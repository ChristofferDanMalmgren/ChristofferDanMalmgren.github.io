[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intuitiv Statistik",
    "section": "",
    "text": "Metadata: före eller efter?\n\n\n\nMetadata\n\nStatistikproduktion\n\nArbetsrutiner\n\n\n\nMina tankar kring metadatadokumentering och metadatastyrning\n\n\n\n\n\nFeb 23, 2026\n\n\nChristoffer Malmgren\n\n\n\n\n\n\n\n\n\n\n\n\nGit (del 1)\n\n\n\nGit\n\nStatistikproduktion\n\nArbetsrutiner\n\n\n\nDen första delen om Git där jag förklarar nyttan av Git i statistikproduktion\n\n\n\n\n\nFeb 9, 2026\n\n\nChristoffer Malmgren\n\n\n\n\n\n\n\n\n\n\n\n\nGit (del 2)\n\n\n\nGit\n\nStatistikproduktion\n\nArbetsrutiner\n\n\n\nDen andra delen om Git där jag förklarar grundläggande men tillräcklig syntax för att börja med Git.\n\n\n\n\n\nFeb 9, 2026\n\n\nChristoffer Malmgren\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Om bloggen",
    "section": "",
    "text": "På en demografikonferens fastnade jag för uttrycket “KISS – Keep it simple, stupid”, när en ovanligt excentrisk men kunnig man med cowboyhatt i publiken sa just det samtidigt som han gestikulerade och gjorde en kyss i luften. Det var ett ögonblick som verkligen satte sig: en enkel och korrekt analys kan vara både intuitiv, tydlig och mest lämplig, även när mer avancerade metoder finns. Sedan dess har jag drivits av samma ambition – att förstå och presentera statistik på ett sätt som är både korrekt och intuitivt.\nDen här bloggen är främst för min egen skull: ett sätt att samla sådant jag snöat in mig på, från state space-tidsseriemodeller, kausal inferens och grafbaserade metoder till Git och kodlösningar. Ofta blir mina anteckningar annars liggande i ostrukturerade dokument. Här kan jag formulera problem och metoder på ett sätt som blir intuitivt för mig – och samtidigt dela det med andra som vill förstå statistiken på ett mer intuitivt sätt.\nHoppas du som läser bloggen hittar något givande här, och du är alltid varmt välkommen att höra av dig!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/DAG-del-1/index.html",
    "href": "posts/DAG-del-1/index.html",
    "title": "DAGs: En introduktion (del 1)",
    "section": "",
    "text": "Figure 1: …\nDAGs, eller Directed Acyclical Graphs, är ett analytiskt verktyg med många användningsområden – kanske mest känt inom kausal inferens på observationsdata. Men de kan även användas inom designbaserad inferens, AI/ML och tidsserieanalys – områden jag kommer skriva mer om under taggen DAG."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#vad-är-en-dag",
    "href": "posts/DAG-del-1/index.html#vad-är-en-dag",
    "title": "En introduktion till DAG – Directed Acyclical Graphs",
    "section": "Vad är en DAG?",
    "text": "Vad är en DAG?\nEn DAG är i grunden ett grafiskt sätt att strukturera och tydliggöra de antaganden man gör om samband mellan variabler. Oftast används det inom kausal inferens på observationsdata, där man vill förstå hur en variabel påverkar en annan, och vilka faktorer som kan störa eller förklara sambandet.\nTanken är enkel:\nDu ritar upp de variabler du vill studera – till exempel en behandling (treatment) och ett utfall (outcome) – och sedan ritar du pilar som representerar hur du tror att dessa påverkar varandra.\nDen färdiga grafen hjälper dig att:\n\nidentifiera vilka variabler du behöver kontrollera för för att undvika bias,\nse vilka variabler du inte bör kontrollera för, eftersom de kan skapa bias,\nförstå vart man kan hitta icke-kausala samband\nförstå och visualisera saker så som selektionsbias och bortfallsbias\n\nI en DAG är pilarna alltid riktade (de går åt ett håll) och grafen får inte innehålla feedbackloopar – därför kallas den acyklisk. En variabel får alltså inte påverka sig själv, varken direkt eller indirekt.\n\nEn enkel kausal relation (ancestors och descendants)\nFör att börja enkelt kan vi tänka oss ett scenario där en behandling (treatment) antas påverka ett utfall (outcome). I en DAG representeras detta med en pil från treatment till outcome:\n\n\n\n\n\n\n\n\nFigure 2: …\n\n\n\n\n\nHär säger pilen helt enkelt:\n\nVi antar att förändringar i treatment leder till förändringar i outcome.\n\nDet här är den mest grundläggande byggstenen i en kausal graf – en direkt kausal relation.\nMen samma logik kan generaliseras till mer komplexa strukturer.\nI en DAG pratar man ofta om variabler som ancestors och descendants.\nI figur 1 så är treatment en ancestor till outcome – och outcome är en descendant till treatment.\n\n\n\n\n\n\n\n\nFigure 3: …\n\n\n\n\n\nMan kan tänka sig att en descendant är en slags brusad eller transformerad version av sin ancestor. Den bär fortfarande information om den, men inte nödvändigtvis på ett direkt eller oförvanskat sätt.\nDet här sättet att tänka – vem som är förälder och vem som är “avkomma” i grafen – hjälper oss att förstå hur information och påverkan flödar i ett system.\nI sin enklaste form, som i exemplet ovan, är flödet rakt och tydligt: treatment → outcome.\nMen i mer komplexa grafer kan samma idé användas för att analysera hur effekter sprids genom flera led, eller hur beroenden kan uppstå mellan variabler som inte påverkar varandra direkt.\nAtt kunna identifiera ancestors och descendants är därför centralt i kausal inferens.\nDet är nämligen dessa relationer som avgör:\n\nvilka vägar i grafen som representerar kausala effekter,\noch vilka som kan skapa förväxling eller bias om man analyserar dem fel.\n\n\n\nOlika typer av samband\nEn DAG kan också användas för att representera olika typer av relationer mellan variabler. Några av de mest centrala är:\n\nMediatorer – variabler som förmedlar effekten från behandling till utfall.\nConfounders – variabler som påverkar både behandling och utfall, och därmed kan skapa skensamband.\nColliders – variabler som påverkas av både behandling och utfall; dessa ska man inte kontrollera för, eftersom det kan öppna falska samband.\n\nFiguren nedan illustrerar dessa tre fall:\n\n\n\n\n\n\n\n\nFigure 4: …\n\n\n\n\n\nAtt kunna känna igen dessa strukturer är avgörande för att veta vilka vägar i grafen som representerar kausal påverkan, och vilka som man vill “blockera” genom att kontrollera för vissa variabler.\n\n\nEffect modifier\nSlutligen kan vi ha variabler som modifierar effekten av en behandling – så kallade effect modifiers. Dessa påverkar inte nödvändigtvis kausaliteten i sig, men de kan avgöra hur stark eller vilken riktning effekten har i olika grupper.\n\n\n\n\n\n\n\n\nFigure 5: …\n\n\n\n\n\nI figuren ser vi att utfallet påverkas av flera faktorer som kan förändra hur stark effekten av treatment blir. Sådana relationer är särskilt viktiga när man analyserar heterogena effekter – till exempel om en behandling fungerar olika bra beroende på ålder, kön eller tidigare erfarenhet. Att ta hänsyn till effect modifiers kan också öka precisionen av den totalt genomsnittliga effekten, öka möjligheten att generalisera den totala effekten till en annan population men effect modifiers kan också vara irrelevant. Allt beror på syftet.\n\n\nFunktionell form\nInnan jag avslutar det här inlägget vill jag prata om det kausala sambandens funktionella form. Det handlar om att en DAG inte förklarar hur variablerna påverkar varandra och samma DAG-graf skulle kunna ha väldigt många (om inte onändligt) tänkbara funktionella former. För att illustrera detta kan vi tänka oss en DAG som kan ses i figur…\n\n\n\n\n\n\n\n\nFigure 6: …\n\n\n\n\n\n\n\n.\nBeroende på målet med din analys kan en DAG hjälpa dig att besvara frågor som:\n\nVilka variabler ska jag ta hänsyn till för att uppskatta en kausal effekt?\nVilka variabler kan jag ta hänsyn till, om jag vill minska osäkerhet utan att skapa bias?\nHur ska jag ta hänsyn till dem – som kontrollvariabler, stratifiering eller via modeller?\n\nDet fina med DAGs är att de tvingar dig att uttrycka dina antaganden grafiskt. Genom att göra det synligt vad du tror om världen blir det också tydligare vad du faktiskt kan säga utifrån din data."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#vägar-genom-en-dag",
    "href": "posts/DAG-del-1/index.html#vägar-genom-en-dag",
    "title": "En introduktion till DAG – Directed Acyclical Graphs (del 1)",
    "section": "Vägar genom en DAG",
    "text": "Vägar genom en DAG\nI förra delen gick vi igenom de tre grundstrukturerna: Chain, Fork och Collider. Nu ska vi titta på hur information faktiskt flödar genom dessa strukturer, och vad som händer om man “betingar” på en variabel.\n\nChain: \\(X \\rightarrow Z \\rightarrow Y\\)\nHär påverkar \\(X\\) variabeln \\(Z\\), som i sin tur påverkar \\(Y\\).\nInformation – eller påverkan – flödar alltså från \\(X\\) till \\(Y\\) genom \\(Z\\).\nOm vi betingar på \\(Z\\) (t.ex. genom att hålla den konstant) bryter vi det här flödet, och sambandet mellan \\(X\\) och \\(Y\\) försvinner. Om vi inte betingar på \\(Z\\), är vägen öppen och det finns ett kausalt samband mellan \\(X\\) och \\(Y\\).\nFiktivt exempel:\n\n\n\n\n\n\n\n\nFigure 6: …\n\n\n\n\n\n\nTänk att vi har ett bekämpningsmedel som minskar skadlig svamptillväxt på växter. Ju mindre svamp en växt har, desto större blir den.\nI det här fallet finns ett kausalt samband mellan mängden bekämpningsmedel och växtens storlek, men det går genom att minska svamptillväxten.\nOm vi tar hänsyn till mängden svamp – till exempel genom att bara jämföra växter med samma nivå av svamptillväxt – kommer sambandet mellan bekämpningsmedel och växtstorlek att försvinna.\nDet beror på att bekämpningsmedlets effekt går via svampen: givet en viss nivå av svamp finns ingen ytterligare variation i växtstorlek som kan förklaras av mängden bekämpningsmedel.\n\n\nFork: \\(X \\leftarrow Z \\rightarrow Y\\)\nHär påverkar \\(Z\\) både \\(X\\) och \\(Y\\). Det finns alltså ett samband mellan \\(X\\) och \\(Y\\) som inte är kausalt, utan beror på deras gemensamma orsak \\(Z\\).\nOm vi inte betingar på \\(Z\\), är vägen mellan \\(X\\) och \\(Y\\) öppen, och information kan flöda längs den vägen.\nOm vi däremot betingar på \\(Z\\), blockeras flödet, och sambandet mellan \\(X\\) och \\(Y\\) försvinner.\nFiktivt exempel:\n\n\n\n\n\n\n\n\n\nFigure 7: …\n\n\n\n\n\nAnta att vi undersöker sambandet mellan kaffekonsumtion och stressnivå. Vi antar att en gemensam orsak, till exempel **arbetsbelastning, påverkar båda – hög arbetsbelastning leder ofta till både mer kaffedrickande och högre stress.\nOm vi bara tittar på kaffekonsumtion och stress, utan att ta hänsyn till arbetsbelastning, kommer vi se ett samband mellan dem. Men om vi jämför personer med samma arbetsbelastning, försvinner sambandet i det här exemplet– kaffet orsakar inte stressen, de samvarierar bara för att båda påverkas av samma faktor.\n\n\nCollider: \\(X \\rightarrow Z \\leftarrow Y\\)\nHär påverkar både \\(X\\) och \\(Y\\) variabeln \\(Z\\). Vägen mellan \\(X\\) och \\(Y\\) är blockerad så länge vi inte betingar på \\(Z\\); det finns alltså inget informationsflöde mellan dem.\nOm vi däremot betingar på \\(Z\\) – till exempel genom att bara titta på en viss nivå av \\(Z\\), eller inkludera den som en kontrollvariabel i en modell – öppnas vägen, och ett samband mellan \\(X\\) och \\(Y\\) kan uppstå även om inget kausalt samband finns mellan dem.\nExempel:\n\n\n\n\n\n\n\n\n\nFigure 8: …\n\n\n\n\n\nTänk att vi studerar sambandet mellan antal timmar man tränar per vecka och hur mycket man äter, men vi tittar bara på personer med en viss kroppsvikt. Både träning och matintag påverkar kroppsvikten – men om vi begränsar oss till personer som väger lika mycket, kan det uppstå ett falskt samband: de som tränar mycket tenderar att äta mer, men i vår grupp med lika vikt kommer de som tränar mer ofta vara de som äter mindre för att vikten ska hållas konstant.\nUtan att vi betingat på vikt finns inget nödvändigt samband mellan träning och matintag, men när vi gör det öppnas vägen och skapar ett icke-kausalt samband.\n\n\nTotal effekt = direkt effekt + indirekta effekter\nI praktiska DAGs, likt kausalitet i verkligen, behöver inte effekten gå en väg utan man kan dela in den totala effekten i direkt effekt, som inte har en mediator mellan sig och utfallet och indirekta effekter: som har en effekt via en eller flera mediators.\n\n\n\n\n\n\n\n\nFigure 9: …\n\n\n\n\n\nOftast är det den totala effekten man vill mäta, eller åtminstånde har som mål att mäta. Men i praktisken är det inte är ovanligt att man försöker dela upp den totala effekten i just direkt och indirekta effkter. Detta görs genom att betinga på mediators i det fall man kan anta att det kausala sambandet ser ut i stil med figur XXXX. Att göra sådan analys kallas mediation analysis och är ett omdiskuterat ämne inom kausal inferens och inget jag tänker diskuttera i detta inlägg. För den vetgirige rekomenderar jag: …"
  },
  {
    "objectID": "posts/DAG-del-1/index.html#vad-en-dag-inte-visar-funktionella-former-och-modellering",
    "href": "posts/DAG-del-1/index.html#vad-en-dag-inte-visar-funktionella-former-och-modellering",
    "title": "En introduktion till DAG – Directed Acyclical Graphs (del 1)",
    "section": "Vad en DAG inte visar: funktionella former och modellering",
    "text": "Vad en DAG inte visar: funktionella former och modellering\nEn viktig poäng som ofta glöms bort är att en DAG inte säger något om den funktionella formen mellan variabler.\nEn pil från X→YX YX→Y betyder bara att XXX påverkar YYY på något sätt — inte hur mycket, inte i vilken riktning, och inte på vilket sätt.\nY=f(X,Z,ε)Y = f(X, Z, )Y=f(X,Z,ε)\nStrukturen ovan (t.ex. X→Y←ZX Y ZX→Y←Z) säger bara att YYY beror på XXX och ZZZ.\nMen hur den beror på dem kan vara linjärt, icke-linjärt, interaktivt, logaritmiskt eller något helt annat.\nDet innebär att även om en DAG talar om vilka variabler som bör justeras för, så krävs fortfarande att man väljer hur man justerar för dem i analysen.\nFel funktionell form (t.ex. en linjär modell när sambandet är starkt icke-linjärt) kan fortfarande ge bias – trots att man justerat för “rätt” variabler enligt grafen.\n\nKort sagt: DAGs beskriver struktur, inte form.\nDe visar vem som påverkar vem, men inte hur påverkan ser ut."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#d-separation-den-formella-grunden",
    "href": "posts/DAG-del-1/index.html#d-separation-den-formella-grunden",
    "title": "En introduktion till DAG – Directed Acyclical Graphs (del 1)",
    "section": "d-separation – den formella grunden",
    "text": "d-separation – den formella grunden\nFör den som vill förstå det mer matematiska fundamentet bakom DAGs, finns begreppet d-separation.\nDet är det formella sättet att avgöra om två variabler är oberoende givet en uppsättning andra.\nTvå variabler XXX och YYY är d-separerade av ZZZ om alla vägar mellan XXX och YYY blockeras när man betingar på ZZZ.\nDet här är grunden för backdoor-kriteriet — och egentligen för hela logiken i DAG-resonemanget.\nReglerna kan sammanfattas så här:\n\n\nEn väg blockeras om den innehåller en chain eller fork där den mittersta variabeln är betingad på:\nX→Z→Y,X←Z→YX Z Y, X Z YX→Z→Y,X←Z→Y\n\nEn väg öppnas om den innehåller en collider\nX→C←YX C YX→C←Y\noch vi betingar på collidern (eller någon av dess descendants).\n\n\nAtt förstå d-separation gör att man kan avgöra vilka beroenden som finns kvar efter justering – och vilka man faktiskt lyckats blockera."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#avslutning-från-antagande-till-analys",
    "href": "posts/DAG-del-1/index.html#avslutning-från-antagande-till-analys",
    "title": "En introduktion till DAG – Directed Acyclical Graphs (del 1)",
    "section": "Avslutning: från antagande till analys",
    "text": "Avslutning: från antagande till analys\nDAGs är kraftfulla eftersom de tvingar oss att synliggöra våra antaganden.\nDe hjälper oss att skilja mellan vad som är teoretiskt möjligt, statistiskt identifierbart, och empiriskt testbart.\nMen de ersätter inte analysen – de kompletterar den.\nDe visar strukturen av världen så som vi tror den ser ut, men det är fortfarande upp till oss att välja rätt modell, form och metod.\nEn DAG kan alltså inte tala om för dig exakt hur du ska skatta en effekt – men den kan tala om vilka effekter som går att skatta överhuvudtaget.\n\nEn statistiker utan ämneskunskap famlar i mörker.\nEn ämnesexpert utan en strukturell modell famlar också.\nTillsammans kan de rita en DAG – och börja se klart."
  },
  {
    "objectID": "posts/DAG-del-2/index.html",
    "href": "posts/DAG-del-2/index.html",
    "title": "DAGs: En introduktion (del 2)",
    "section": "",
    "text": "I första delen gick jag igenom de grundläggande byggstenarna i en DAG –\nhur vi kan representera orsakssamband med pilar, och hur olika vägar mellan variabler\nkan blockeras eller öppnas beroende på vilka vi betingar på, vilket kan leda till att kausala effekter kan skattas utan bias.\nDen här delen börjar med hur man kan resonera med DAGs i mer komplexa, verklighetsnära exempel. Sedan diskuterar jag vad DAGs inte kan visa – och avslutar med den matematiska grunden för hur man formellt kan uttrycka kausala antaganden."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#vägar-genom-en-dag",
    "href": "posts/DAG-del-2/index.html#vägar-genom-en-dag",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Vägar genom en DAG",
    "text": "Vägar genom en DAG\nI förra delen gick vi igenom de tre grundstrukturerna: Chain, Fork och Collider. Nu ska vi titta på hur information faktiskt flödar genom dessa strukturer, och vad som händer om man “betingar” på en variabel.\n\nChain: \\(X \\rightarrow Z \\rightarrow Y\\)\nHär flödar information från \\(X\\) till \\(Y\\) via \\(Z\\). Variabeln \\(Z\\) fungerar som en mediator – den förmedlar effekten från orsak (\\(X\\)) till utfall (\\(Y\\)).\nOm vi betingar på \\(Z\\), bryter vi flödet mellan \\(X\\) och \\(Y\\). Utan betingning finns ett samband mellan \\(X\\) och \\(Y\\), men det är helt förmedlat av \\(Z\\).\nFiktivt exempel:\n\n\n\n\n\n\n\n\nFigure 1: …\n\n\n\n\n\n\nTänk att vi har ett bekämpningsmedel som minskar skadlig svamptillväxt på växter. Ju mindre svamp en växt har, desto större blir den.\nI det här fallet finns ett kausalt samband mellan mängden bekämpningsmedel och växtens storlek, men det går genom att minska svamptillväxten.\nOm vi tar hänsyn till mängden svamp – till exempel genom att bara jämföra växter med samma nivå av svamptillväxt – kommer sambandet mellan bekämpningsmedel och växtstorlek att försvinna.\nDet beror på att bekämpningsmedlets effekt går via svampen: givet en viss nivå av svamp finns ingen ytterligare variation i växtstorlek som kan förklaras av mängden bekämpningsmedel.\n\n\nFork: \\(X \\leftarrow Z \\rightarrow Y\\)\nHär påverkar \\(Z\\) både \\(X\\) och \\(Y\\). Det finns alltså ett samband mellan \\(X\\) och \\(Y\\) som inte är kausalt, utan beror på deras gemensamma orsak \\(Z\\).\nOm vi inte betingar på \\(Z\\), är vägen mellan \\(X\\) och \\(Y\\) öppen, och information kan flöda längs den vägen.\nOm vi däremot betingar på \\(Z\\), blockeras flödet, och sambandet mellan \\(X\\) och \\(Y\\) försvinner.\nFiktivt exempel:\n\n\n\n\n\n\n\n\n\nFigure 2: …\n\n\n\n\n\nAnta att vi undersöker sambandet mellan kaffekonsumtion och stressnivå. Vi antar att en gemensam orsak, till exempel **arbetsbelastning, påverkar båda – hög arbetsbelastning leder ofta till både mer kaffedrickande och högre stress.\nOm vi bara tittar på kaffekonsumtion och stress, utan att ta hänsyn till arbetsbelastning, kommer vi se ett samband mellan dem. Men om vi jämför personer med samma arbetsbelastning, försvinner sambandet i det här exemplet– kaffet orsakar inte stressen, de samvarierar bara för att båda påverkas av samma faktor.\n\n\nCollider: \\(X \\rightarrow Z \\leftarrow Y\\)\nHär påverkar både \\(X\\) och \\(Y\\) variabeln \\(Z\\). Vägen mellan \\(X\\) och \\(Y\\) är blockerad så länge vi inte betingar på \\(Z\\); det finns alltså inget informationsflöde mellan dem.\nOm vi däremot betingar på \\(Z\\) – till exempel genom att bara titta på en viss nivå av \\(Z\\), eller inkludera den som en kontrollvariabel i en modell – öppnas vägen, och ett samband mellan \\(X\\) och \\(Y\\) kan uppstå även om inget kausalt samband finns mellan dem.\nExempel:\n\n\n\n\n\n\n\n\n\nFigure 3: …\n\n\n\n\n\nTänk att vi studerar sambandet mellan antal timmar man tränar per vecka och hur mycket man äter, men vi tittar bara på personer med en viss kroppsvikt. Både träning och matintag påverkar kroppsvikten – men om vi begränsar oss till personer som väger lika mycket, kan det uppstå ett falskt samband: de som tränar mycket tenderar att äta mer, men i vår grupp med lika vikt kommer de som tränar mer ofta vara de som äter mindre för att vikten ska hållas konstant.\nUtan att vi betingat på vikt finns inget nödvändigt samband mellan träning och matintag, men när vi gör det öppnas vägen och skapar ett icke-kausalt samband.\n\n\nTotal effekt = direkt effekt + indirekta effekter\nI praktiska DAGs, likt kausalitet i verkligen, behöver inte effekten gå en väg utan man kan dela in den totala effekten i direkt effekt, som inte har en mediator mellan sig och utfallet och indirekta effekter: som har en effekt via en eller flera mediators.\n\n\n\n\n\n\n\n\nFigure 4: …\n\n\n\n\n\nFiguren visar:\n\nDirekt effekt: treatment -&gt; outcome\nindirekt effekt: treatment -&gt; mediator -&gt; outcome\ntotal effekt: direkt effekt + indirekt effekt\n\nOftast är det den totala effekten man vill mäta, eller åtminstånde har som mål att mäta. Men i praktisken är det inte är ovanligt att man försöker dela upp den totala effekten i just direkt och indirekta effkter. Detta görs genom att betinga på mediators i det fall man kan anta att det kausala sambandet ser ut i stil med figur XXXX. Att göra sådan analys kallas mediation analysis och är ett omdiskuterat ämne inom kausal inferens och inget jag tänker diskuttera i detta inlägg. För den vetgirige rekomenderar jag: …"
  },
  {
    "objectID": "posts/DAG-del-2/index.html#vad-en-dag-inte-visar-funktionella-former-och-modellering",
    "href": "posts/DAG-del-2/index.html#vad-en-dag-inte-visar-funktionella-former-och-modellering",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Vad en DAG inte visar: funktionella former och modellering",
    "text": "Vad en DAG inte visar: funktionella former och modellering\nEn viktig poäng som ofta glöms bort är att en DAG inte säger något om den funktionella formen mellan variabler.\nEn pil från X→YX YX→Y betyder bara att XXX påverkar YYY på något sätt — inte hur mycket, inte i vilken riktning, och inte på vilket sätt.\nY=f(X,Z,ε)Y = f(X, Z, )Y=f(X,Z,ε)\nStrukturen ovan (t.ex. X→Y←ZX Y ZX→Y←Z) säger bara att YYY beror på XXX och ZZZ.\nMen hur den beror på dem kan vara linjärt, icke-linjärt, interaktivt, logaritmiskt eller något helt annat.\nDet innebär att även om en DAG talar om vilka variabler som bör justeras för, så krävs fortfarande att man väljer hur man justerar för dem i analysen.\nFel funktionell form (t.ex. en linjär modell när sambandet är starkt icke-linjärt) kan fortfarande ge bias – trots att man justerat för “rätt” variabler enligt grafen.\n\nKort sagt: DAGs beskriver struktur, inte form.\nDe visar vem som påverkar vem, men inte hur påverkan ser ut."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#d-separation-den-formella-grunden",
    "href": "posts/DAG-del-2/index.html#d-separation-den-formella-grunden",
    "title": "DAGs: En introduktion (del 2)",
    "section": "d-separation – den formella grunden",
    "text": "d-separation – den formella grunden\nFör den som vill förstå det mer matematiska fundamentet bakom DAGs, finns begreppet d-separation.\nDet är det formella sättet att avgöra om två variabler är oberoende givet en uppsättning andra.\nTvå variabler XXX och YYY är d-separerade av ZZZ om alla vägar mellan XXX och YYY blockeras när man betingar på ZZZ.\nDet här är grunden för backdoor-kriteriet — och egentligen för hela logiken i DAG-resonemanget.\nReglerna kan sammanfattas så här:\n\n\nEn väg blockeras om den innehåller en chain eller fork där den mittersta variabeln är betingad på:\nX→Z→Y,X←Z→YX Z Y, X Z YX→Z→Y,X←Z→Y\n\nEn väg öppnas om den innehåller en collider\nX→C←YX C YX→C←Y\noch vi betingar på collidern (eller någon av dess descendants).\n\n\nAtt förstå d-separation gör att man kan avgöra vilka beroenden som finns kvar efter justering – och vilka man faktiskt lyckats blockera."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#avslutning-från-antagande-till-analys",
    "href": "posts/DAG-del-2/index.html#avslutning-från-antagande-till-analys",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Avslutning: från antagande till analys",
    "text": "Avslutning: från antagande till analys\nDAGs är kraftfulla eftersom de tvingar oss att synliggöra våra antaganden.\nDe hjälper oss att skilja mellan vad som är teoretiskt möjligt, statistiskt identifierbart, och empiriskt testbart.\nMen de ersätter inte analysen – de kompletterar den.\nDe visar strukturen av världen så som vi tror den ser ut, men det är fortfarande upp till oss att välja rätt modell, form och metod.\nEn DAG kan alltså inte tala om för dig exakt hur du ska skatta en effekt – men den kan tala om vilka effekter som går att skatta överhuvudtaget."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#hur-jag-kom-i-kontakt-med-dags",
    "href": "posts/DAG-del-1/index.html#hur-jag-kom-i-kontakt-med-dags",
    "title": "DAGs: En introduktion (del 1)",
    "section": "Hur jag kom i kontakt med DAGs",
    "text": "Hur jag kom i kontakt med DAGs\nMin introduktion till DAGs kom faktiskt inte via studier, utan genom boken Statistical Rethinking, 2nd Edition av Richard McElreath.\nJag hade tidigare läst första upplagan, som jag tyckte var en fantastisk introduktion till bayesiansk statistik: lättförståelig, intuitiv och med fokus på förståelse snarare än formalia.\nNär jag såg att den nya versionen lade mer fokus på kausal inferens och tidsseriemodellering blev jag direkt nyfiken.\nDet var där jag första gången verkligen förstod kraften i DAGs. Plötsligt fick saker jag redan kände till – som omitted variable bias, instrumentvariabelansats och bortfallsbias – en tydligare plats i ett gemensamt ramverk: DAG.\n\nVad är en DAG?\nEn DAG är ett grafiskt sätt att strukturera och tydliggöra de antaganden man gör om samband mellan variabler. Vanligtvis används den inom kausal inferens för att förstå hur en variabel påverkar en annan, och vilka faktorer som kan störa eller förklara sambandet.\nMan ritar upp variabler och drar pilar som representerar antagna kausala samband. Pilarna är alltid riktade (\\(\\rightarrow\\)), och grafen får inte innehålla feedbackloopar (acyklisk), varken direkt eller indirekt.\nEn DAG hjälper oss, bland annat, att:\n\nIdentifiera vilka variabler vi behöver kontrollera, eller undvika att kontrollera för, så att vi kan undvika att få skattningar med bias.\nFörstå var icke-kausala samband kan uppstå.\nVisualisera fenomen som selektionsbias och bortfallsbias."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#enkla-kausala-relationer",
    "href": "posts/DAG-del-1/index.html#enkla-kausala-relationer",
    "title": "DAGs: En introduktion (del 1)",
    "section": "Enkla kausala relationer",
    "text": "Enkla kausala relationer\nTänk dig en behandling \\(X\\) som påverkar ett utfall \\(Y\\):\n\\[\nX \\rightarrow Y\n\\]\nHär representerar pilen den direkta effekten från \\(X\\) till \\(Y\\).\nMan kan också prata om ancestors och descendants. Där \\(X\\) är en ancestor till \\(Y\\), och \\(Y\\) är en descendant till \\(X\\): \\[\nancestor \\rightarrow descendant\n\\] Denna relation kan ses som en kausal effekt mellan \\(X\\) och \\(Y\\) eller att \\(Y\\) är en funktion av \\(X\\) på något sätt: \\(Y=f(X)\\)"
  },
  {
    "objectID": "posts/DAG-del-1/index.html#grundläggande-vägar-chain-fork-och-collider",
    "href": "posts/DAG-del-1/index.html#grundläggande-vägar-chain-fork-och-collider",
    "title": "DAGs: En introduktion (del 1)",
    "section": "Grundläggande vägar: Chain, Fork och Collider",
    "text": "Grundläggande vägar: Chain, Fork och Collider\n\nChain (mediation)\n\\[\nX \\rightarrow Z \\rightarrow Y\n\\]\nHär fungerar \\(Z\\) som en mediator. Effekten från \\(X\\) till \\(Y\\) går alltså via \\(Z\\).\n\n\\(Z\\) fungerar som mediator mellan \\(X\\) och \\(Y\\).\n\nOm vi betingar på \\(Z\\) blockeras det kausala flödet från \\(X\\) till \\(Y\\).\n\n\nFiktivt exempel: \\[\nbekämpningsmedel \\rightarrow mängd \\ skadlig \\ svamp \\rightarrow storlek \\ på \\ växt\n\\] Här antar vi det kausala sambandet att bekämpningsmedel minskar mängden skadlig svamp och ju mindre svamp en växt har desto större blir den. Om vi håller svampmängden konstant (betingar på svampmängd), kommer vi inte längre se något samband mellan bekämpningsmedel och storlek på växt – eftersom bekämpningsmedlet påverkar växtens storlek enbart genom att minska svampen.\n\n\n\nFork (confounding)\n\\[\nX←Z→Y\n\\]\n\n\\(Z\\) är en confounder – en gemensam orsak till \\(X\\) och \\(Y\\).\nAtt betinga på \\(Z\\) blockerar flödet mellan \\(X\\) och \\(Y\\).\n\n\nFiktivt exempel: \\[\nelförbrukning \\leftarrow utetemperatur \\rightarrow glassförsäljning\n\\] Här antar vi att utetemperatur påverkar både elförbrukning och glassförsäljning. Om vi inte tar hänsyn till temperaturen, ser det ut som att elförbrukning och glassförsäljning hänger ihop, men det beror på att de båda beror på utetemperaturen.\n\n\n\nCollider\n\\[\nX→Z←Y\n\\] - \\(Z\\) är en collider.Flödet mellan \\(X\\) och \\(Y\\) är blockerat om vi inte betingar på \\(Z\\). - Att betinga på \\(Z\\) eller dess descendants öppnar vägen och kan skapa ett falskt samband.\n\nFiktivt exempel: \\[\nbetyg \\rightarrow antagen \\ till \\ program \\leftarrow rekommendationer\n\\] Här antar vi att betyg och starka rekommendationer påverkar chansen att bli antagen till ett prestigefyllt program. Bland de som blir antagna (betingar på de antagna) ser man ett icke-kausalt mönster: personer med lägre betyg tenderar att ha starkare rekommendationer, och vice versa, trots att betyg och rekommendationer kan vara oberoende i populationen.\n\n\nDessa tre strukturer – Chain (mediator), Fork (confounder) och Collider (collider) – är grunden för hur vi kan resonera om flöden i en DAG.\n\nAlla mer komplexa grafer är bara kombinationer av dessa mönster.\n\nGenom att förstå när vägar är öppna eller blockerade kan vi se vilka relationer som faktiskt förmedlar information, och vilka som bara verkar göra det."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#total-effekt-direkt-effekt-indirekta-effekter",
    "href": "posts/DAG-del-1/index.html#total-effekt-direkt-effekt-indirekta-effekter",
    "title": "DAGs: En introduktion (del 1)",
    "section": "Total effekt = direkt effekt + indirekta effekter",
    "text": "Total effekt = direkt effekt + indirekta effekter\nI exemplet med chain såg vi att effekten från \\(X\\) till \\(Y\\) gick via \\(Z\\). Det leder oss in på ett centralt begrepp inom kausal inferens: den totala effekten kan delas upp i en direkt del och en indirekt del.\nI praktiska DAGs – precis som i verkligheten – kan en orsak ha en effekt genom flera mekansimer. Den totala effekten av en variabel på en annan kan delas upp i två delar:\n\nDirekt effekt: påverkan från orsak till utfall utan någon mediator emellan.\nIndirekta effekter: påverkan som går genom en eller flera mediatorer.\n\nVi kan illustrera detta med följande DAG: \\[\n\\begin{array}{c}\n& Z & \\\\\n& \\nearrow \\searrow &\\\\\n&X \\rightarrow Y&\n\\end{array}\n\\] I DAGen ovan så är:\n\ndirekt effekt: \\(X \\rightarrow Y\\)\nindirekt effekt: \\(X \\rightarrow Z \\rightarrow Y\\)\ntotal effekt: summan av direkt och inderekt effekt\n\nOftast är det den totala effekten man vill skatta – eller åtminstone ha som utgångspunkt. Men i många tillämpningar vill man även förstå hur effekten uppstår, och då kan man försöka dela upp den i sina beståndsdelar. Det görs genom att betinga på mediatorer, under antagandet att den kausala strukturen ser ut ungefär som i figuren ovan.\nDen typen av analys kallas mediation analysis och är ett etablerat (och delvis omdiskuterat) område inom kausal inferens. Jag kommer inte gå in på detaljer här, men för den som vill fördjupa sig rekommenderas till exempel:\n….Pearl, J. (2001). Direct and Indirect Effects.\n…VanderWeele, T. (2015). Explanation in Causal Inference: Methods for Mediation and Interaction.\n\nDet är viktigt att komma ihåg att en antagen kausal effekt mellan \\(treatment\\) och \\(outcome\\) inte betyder att man med säkerhet kommer observera en faktisk effekt.\nEn korrekt specificerad modell – där man antar ett kausalt samband – kan mycket väl innebära att den totala effekten är noll.\nDet kan ske om den direkta och indirekta effekten motverkar varandra,\neller om det antagna orsakssambandet helt enkelt inte finns i verkligheten.\nMed andra ord - vi kan se kausal inferens som ett test av en tänkt typ av orsakssamband – inte som ett bevis på att det faktiskt måste finnas en effekt.\nAtt skatta en nolleffekt är därför lika mycket en kausal slutsats\nsom att skatta en positiv eller negativ effekt.\n\nEffect modifier\nUtöver de tre ovanstående strukturerna kan vi ha variabler som modifierar effekten av en behandling – så kallade effect modifiers.\nDessa påverkar inte nödvändigtvis kausaliteten i sig, men de kan avgöra hur stark effekten blir eller i vilken riktning effekten verkar, i olika grupper. \\[\n\\begin{array}{c}\n& effect \\ modifier_1 & \\\\\n& \\downarrow & \\\\\ntreatment & \\longrightarrow outcome \\longleftarrow & effect \\ modifier_2\\\\\n&\\uparrow&\\\\\n& effect \\ modifier_3&\n\\end{array}\n\\]\nI DAGen ser vi att utfallet påverkas av flera faktorer som kan förändra hur stark effekten av treatment blir eller dess riktning.\nSådana relationer är särskilt viktiga när man analyserar heterogena effekter – till exempel om en behandling fungerar olika bra beroende på ålder, kön eller tidigare erfarenhet. För att skatta den genomsnittliga totala effekter är de oftast inte relevanta.\nMen att ta hänsyn till effect modifiers kan:\n\nöka precisionen i den genomsnittliga effekten,\nförbättra möjligheten att generaliseras till en annan population,\nmen i vissa samanhang är de irrelevanta – allt beror på syftet med analysen.\n\n\n\nAvslutande ord\nI det här inlägget har jag gått igenom de mest grundläggande byggstenarna i en DAG – från enkla kausala relationer till hur vägar kan blockeras eller öppnas beroende på vilka variabler vi betingar på.\nPoängen är inte bara att rita pilar, utan att börja tänka strukturellt om orsak och verkan. När man väl gör det, blir det tydligt att statistik inte handlar om att “hitta samband”, utan om att förstå vilka samband som faktiskt betyder något.\nDet jag själv fastnade för när jag började lära mig mer om DAGs var hur de tvingar en\natt vara ärlig med sina antaganden. Det går inte att gömma sig bakom ett regressionsuttryck\neller en modell – man måste faktiskt visa vad man tror påverkar vad. Det i sin tur gör analysen både tydligare och mer ödmjuk.\nJag tycker också DAGs visar på vikten av ämneskunskap i kombination med statistisk metodkunskap.\nUtan ämneskunskap är pilarna och vad man tar med i sin analys bara gissningar.\nDet är först när man kombinerar statistiska verktyg med förståelse för fenomenet\nsom statistik blir meningsfull."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#dags-i-mer-realistiska-situationer",
    "href": "posts/DAG-del-2/index.html#dags-i-mer-realistiska-situationer",
    "title": "DAGs: En introduktion (del 2)",
    "section": "DAGs i mer realistiska situationer",
    "text": "DAGs i mer realistiska situationer\n\nExempel 1\nI det här exemplet antar vi följande kausala struktur:\n\n\n\n\n\n\n\n\nFigure 1: …\n\n\n\n\n\nFrån figuren kan vi utläsa att treatment antas ha en kausal påverkan på outcome, en direkt och en indirekt väg:\n\ndirekt väg: \\(treatment \\rightarrow outcome\\)\nindirekt väg: \\(treatment \\rightarrow A \\rightarrow outcome\\)\n\nVi kan också se att vi har en chain (\\(treatment \\rightarrow A \\rightarrow outcome\\)), en collider som inte innefattar \\(outcome\\) i mitten (\\(treatment \\rightarrow A \\leftarrow B\\)), en fork (\\(A \\leftarrow B \\rightarrow outcome\\)) och en effect modifier \\(C\\).\n\nDet här fallet skulle kunna illustrera ett klassiskt experiment - random control trial (RCT) - då vi inte har något som påverkar treatment.\n\nSå här resonerar jag kring hur man kan skatta en eventuell kausal effekt i det här fallet:\n\nOm vi är ute efter att skatta den genomsnittliga totala effekten:\n\nVi ska inte betinga på mediatorn A, av just det skälet att vi inte är intresserade av att dela upp den totala effekten i dess komponenter.\nDå vi inte har betingat på A så gör det varken till eller ifrån om vi betingar på B för inget flöde kan gå från treatment till outcome via B just för att vi inte betingat på A, vilket fungerar som en collider här.\nAtt betinga på C är inte nödvändigt för syftet\n\nOm vi är intresserade av att dela upp den genomsnittliga totala effekten i direkt och indirekt effekt (mediation analys):\n\nVi ska betinga på A för att ens kunna skatta de olika effekterna.\nDetta ger oss figuren nedan.\n\n\n\n\n\n\n\n\n\nFigure 2: …\n\n\n\n\n\n\nDå vi nu har betingat på A så måste vi betinga på B, annars kommer det fungera som en confounder mellan mediatorn A och outcome: den indirekta effekten skulle då inkludera ett skenbart samband.\nSom tidigare så är det inte nödvändigt för syftet att betinga på C\n\n\n\n\nExempel 2"
  },
  {
    "objectID": "posts/DAG-del-2/index.html#matematisk-representation-av-dags",
    "href": "posts/DAG-del-2/index.html#matematisk-representation-av-dags",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Matematisk representation av DAGs",
    "text": "Matematisk representation av DAGs\nEn DAG kan alltid skrivas som en gemensam sannolikhetsfördelning med hjälp av kedjeregeln:\n\\[\nP(X_1, X_2, \\dots, X_n) = \\prod_i P(X_i \\mid \\text{Parents}(X_i))\n\\]\nVarje variabel beror alltså bara på sina föräldrar i grafen.\n\nI vanlig statistisk analys studerar man ofta \\(P(Y \\mid X)\\) – alltså hur \\(Y\\) varierar med \\(X\\) i observerade data.\nMen det är bara ett beskrivande samband, inte ett orsakssamband. För att beskriva vad som skulle hända om vi faktiskt ändrade \\(X\\) behöver vi istället använda:\n\\[\nP(Y \\mid do(X))\n\\]\ndär \\(do(X)\\) innebär att vi aktivt sätter \\(X\\) till ett värde. Skillnaden blir:\n\\[\nP(Y \\mid do(X)) \\neq P(Y \\mid X)\n\\]\nDen första beskriver ett interventionsscenario, medan den andra beskriver ett observerat samband.\nDet här är kärnan i övergången från korrelation till kausalitet.\n\nFör att kunna identifiera en total kausal effekt måste vi justera för alla icke-kausala samband mellan \\(X\\) och \\(Y\\).\nDet gör vi genom den så kallade bakdörrsjusteringen.\nMålet är att hitta en uppsättning variabler \\(Z\\) som blockerar alla vägar som börjar med pilar in i \\(X\\).\nSådana vägar representerar confounding-effekter som kan skapa skenbara samband.\nNär dessa bakdörrar är blockerade – det vill säga, när vi brutit alla icke-kausala vägar in i \\(X\\) – gäller att \\(X\\) och \\(Y\\) är oberoende givet \\(Z\\) i en graf där vi har tagit bort alla pilar från \\(X\\).\nVi betecknar denna modifierade graf som \\(G_{\\overline{X}}\\).\n\\[\nY \\perp\\!\\!\\!\\perp X \\mid Z\n\\quad \\text{i grafen } G_{\\overline{X}}\n\\]\nvilket är, i en sådan graf, ekvivalent med:\n\\[\np(y,x \\mid z) = p(y \\mid z)\\, p(x \\mid z)\n\\]\nDet betyder att \\(X\\) och \\(Y\\) blir oberoende givet \\(Z\\) efter att vi stängt dörrarna bakåt – alltså i grafen \\(G_{\\overline{X}}\\) där orsakspilarna från \\(X\\) tagits bort.\nMen i den ursprungliga grafen (där vägar både in i och ut från \\(X\\) fortfarande finns) ska samma betingning inte göra dem oberoende – annars finns ingen kausal effekt att identifiera:\n\\[\np(y \\mid z)\\, p(x \\mid z) \\neq p(y,x \\mid z)\n\\]\n\nNär villkoret ovan är uppfyllt kan sambandet mellan \\(X\\) och \\(Y\\) uttryckas i observerbara sannolikheter genom bakdörrsformeln:\n\\[\np(y \\mid do(x)) = \\sum_z p(y \\mid x,z)\\, p(z)\n\\]\nDetta är en central identitetsformel i kausal inferens.\nDen visar att vi kan beräkna en kausal effekt utan att utföra ett faktiskt experiment –\nså länge vi har justerat för rätt uppsättning variabler \\(Z\\) som blockerar alla bakvägar.\n\nEn viktig distinktion är att vägar från \\(X\\) inte behöver blockeras för att identifiera den totala effekten – men de kan bli relevanta om vi vill dela upp effekten i en direkt och en indirekt del (mediation analysis),\neller om man försöker skatta den totala effekten i situationer där bakvägar inte går att blockera.\nDessa scenarier kräver ofta starkare antaganden, men grunden är densamma:\natt vi genom grafen kan avgöra vilka oberoenden som måste gälla för att identifiera en kausal effekt."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#vad-en-dag-inte-visar",
    "href": "posts/DAG-del-2/index.html#vad-en-dag-inte-visar",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Vad en DAG inte visar",
    "text": "Vad en DAG inte visar\nEn DAG beskriver strukturella antaganden om kausala relationer,\nmen den säger ingenting om hur starka dessa relationer är, eller hur de ser ut matematiskt.\nDen visar alltså att något påverkar något – inte hur eller hur mycket.\nPilen \\(X \\rightarrow Y\\) betyder inte att effekten är linjär, konstant eller ens positiv.\nDen betyder bara att \\(X\\) påverkar \\(Y\\) på något sätt. Är det flera variabler som påverkar \\(Y\\) kan dessa interagera på ett komplext sätt.\nDet här är lätt att glömma när man arbetar med verkliga data –\noch något jag tycker det pratades om alldeles för lite i min utbildning. Jag har sett det här missförståndet dyka upp i forskning och praktiska tillämpningar:\natt man “tar hänsyn till” olika variabler genom att lägga till dem additivt i en linjär regressionsmodell, utan att reflektera över om modellen verkligen motsvarar den kausala struktur man tänker sig, både i termer av vad man har med och variablernas funktionella form.\nEn korrekt ritad DAG, där man tar hänsyn till de variabler man behöver, kan alltså ändå leda till fel slutsatser om man gör fel antaganden om den funktionella formen –\ndet vill säga hur \\(Y\\) faktiskt beror på \\(X\\) och andra variabler, exempelvis \\(Z\\) och \\(W\\):\n\\[\nY = f(X, Z, W)\n\\]\nTvå situationer kan ha exakt samma kausala struktur men helt olika form på sambandet.\nI den ena kanske sambandet är linjärt:\n\\[\nY = \\alpha + \\beta X + \\varepsilon\n\\]\nmedan det i den andra är icke-linjärt:\n\\[\nY = \\alpha + \\beta_1 X + \\beta_2 X^2 + \\varepsilon\n\\]\nDAGen skulle se identisk ut i båda fallen, men effekten av \\(X\\) är helt olika."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#dags-i-mer-komplexa-situationer",
    "href": "posts/DAG-del-2/index.html#dags-i-mer-komplexa-situationer",
    "title": "DAGs: En introduktion (del 2)",
    "section": "DAGs i mer komplexa situationer",
    "text": "DAGs i mer komplexa situationer\nFöljande tre exempel är DAGs som är en kombination av de grundläggande strukturerna vi pratade om i del 1. Dessa kommer innehålla abstrakta symboler då jag känner att det viktigaste är bara att illustrera hur man kan resonera kring öppna och stängda vägar, bias och flöden i mer komplexa situationer.\nExempel 1 och 2 kan illustrera fiktiva experiment - random control trial (RCT) - då vi i dessa exempel inte har något som påverkar treatment. Exempel 3 skulle kunna vara ett exempel på en observationsstudie där vi har saker i modellen som påverkar treatment.\n\nExempel 1\nI det här exemplet antar vi följande kausala struktur:\n\n\n\n\n\n\n\n\nFigure 1: …\n\n\n\n\n\nFrån figuren kan vi utläsa att treatment antas ha en kausal påverkan på outcome: en direkt och en indirekt effekt:\n\ndirekt effekt: \\(treatment \\rightarrow outcome\\)\nindirekt effekt: \\(treatment \\rightarrow A \\rightarrow outcome\\)\n\nVi kan också se att vi har en chain (\\(treatment \\rightarrow A \\rightarrow outcome\\)), en collider som inte innefattar \\(outcome\\) i mitten (\\(treatment \\rightarrow A \\leftarrow B\\)), en fork (\\(A \\leftarrow B \\rightarrow outcome\\)) och en effect modifier \\(C\\).\nSå här kan vi resonera för att förstå hur en eventuell kausal effekt kan skattas i det här fallet:\n\nOm vi är ute efter att skatta den genomsnittliga totala effekten:\n\nVi ska inte betinga på mediatorn A, av just det skälet att vi inte är intresserade av att dela upp den totala effekten i dess komponenter.\nDå vi inte har betingat på A så gör det varken till eller ifrån om vi betingar på B för inget flöde kan gå från treatment till outcome via B just för att vi inte betingat på A, vilket fungerar som en collider här.\nAtt betinga på C är inte nödvändigt för syftet.\n\nOm vi är intresserade av att dela upp den genomsnittliga totala effekten i direkt och indirekt effekt (mediation analys):\n\nVi ska betinga på A för att ens kunna skatta de olika effekterna.\nDå vi nu har betingat på A så måste vi betinga på B, annars kommer det fungera som en confounder mellan mediatorn A och outcome och bidra med ett skenbart, icke-kausalt samband.\nSom tidigare så är det inte nödvändigt för syftet att betinga på C.\n\n\n\n\nExempel 2\nI förra exemplet kom vi fram till att:\n\nom vi ska skatta den totala effekten får vi inte betinga på mediatorn A\nom vi vill skatta den direkta och indirekta effekten ska vi:\n\nbetinga på A och betinga på B\n\n\nHär vill jag bara illustrera vad det skulle resultera i om det i praktiken inte fanns några kausala väger mellan treatment och outcome som figuren nedan visar\n\n\n\n\n\n\n\n\nFigure 2: …\n\n\n\n\n\nom vi försökte skatta den totala effekten genom att inte betinga på något skulle vi göra rätt: vi skulle i genomsnitt skatta den totala effekten till 0.\nOm vi försökte skatta direkt och indirekt effekt genom att betinga på både A och B skulle vi också göra rätt. Här är det extra tydligt att om vi skulle betinga på mediatorn A så skulle vi öppna upp ett skenbart samband treatment och outcome via A och B.\n\n\nExempel 3\nI det här exemplet antar vi följande kausala struktur:\n\n\n\n\n\n\n\n\nFigure 3: …\n\n\n\n\n\nFrån figuren kan vi utläsa att treatment antas ha en kausal påverkan på outcome, en direkt och en indirekt effekt:\n\ndirekt effekt: \\(treatment \\rightarrow outcome\\)\nindirekt effekt: \\(treatment \\rightarrow C \\rightarrow outcome\\)\n\nVi kan också se att vi har en chain (\\(treatment \\rightarrow C \\rightarrow outcome\\)), ingen collider som inte innefattar \\(outcome\\) eller treatment i mitten en fork (\\(treatment \\leftarrow B \\rightarrow outcome\\)) och en effect modifier \\(A\\).\nSå här kan vi resonera för att förstå hur en eventuell kausal effekt kan skattas i det här fallet:\n\nOm vi är ute efter att skatta den genomsnittliga totala effekten:\n\nVi ska inte betinga på mediatorn C, av just det skälet att vi inte är intresserade av att dela upp den totala effekten i dess komponenter.\nför att inte sambandet mellan treatment och outcome ska innehålla ett skenbart samband måste vi betinga på B och stänga forken\nAtt betinga på A är inte nödvändigt för syftet.\n\nOm vi är intresserade av att dela upp den genomsnittliga totala effekten i direkt och indirekt effekt (mediation analys) är den enda skillnaden att vi ska betinga på C"
  },
  {
    "objectID": "posts/DAG-del-2/index.html#från-korrelation-till-kausalitet",
    "href": "posts/DAG-del-2/index.html#från-korrelation-till-kausalitet",
    "title": "DAGs: En introduktion (del 2)",
    "section": "2. Från korrelation till kausalitet",
    "text": "2. Från korrelation till kausalitet\nI vanliga modeller tittar vi på \\(P(Y \\mid X)\\) – sambandet mellan \\(Y\\) och \\(X\\) i observerade data.\nMen detta säger inget om hur världen skulle se ut om vi ändrade \\(X\\).\nFör det behöver vi istället:\n\\[\nP(Y \\mid do(X))\n\\]\ndär \\(do(X)\\) innebär att vi aktivt sätter \\(X\\) till ett värde och därmed bryter alla pilar in till \\(X\\). Skillnaden kan formuleras så här:\n\\[\nP(Y \\mid do(X)) \\neq P(Y \\mid X)\n\\]\neftersom den första beskriver ett interventionsscenario, medan den andra beskriver ett observerat samband."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#identifiering-med-bakdörrsjustering",
    "href": "posts/DAG-del-2/index.html#identifiering-med-bakdörrsjustering",
    "title": "DAGs: En introduktion (del 2)",
    "section": "3. Identifiering med bakdörrsjustering",
    "text": "3. Identifiering med bakdörrsjustering\nFör att skatta den totala kausala effekten av \\(X\\) på \\(Y\\):\n\nBlockera alla icke-kausala vägar som går in i \\(X\\) (bakdörrar) med en uppsättning variabler \\(Z\\).\n\nVägar som går ut från \\(X\\) (framåtriktade icke-kausala vägar) påverkar normalt inte den totala effekten, men blir relevanta vid mediation eller front-door justering.\n\nOm bakdörrar är korrekt blockerade gäller:\n\\[\nY \\perp\\!\\!\\!\\perp X \\mid Z \\quad \\Leftrightarrow \\quad p(y,x \\mid z) = p(y \\mid x,z)\\, p(x \\mid z)\n\\]"
  },
  {
    "objectID": "posts/DAG-del-2/index.html#från-betingad-sannolikhet-till-kausal-effekt",
    "href": "posts/DAG-del-2/index.html#från-betingad-sannolikhet-till-kausal-effekt",
    "title": "DAGs: En introduktion (del 2)",
    "section": "4. Från betingad sannolikhet till kausal effekt",
    "text": "4. Från betingad sannolikhet till kausal effekt\nNär bakdörrar är blockerade kan vi skriva:\n\\[\np(y \\mid do(x)) = \\sum_z p(y \\mid x,z)\\, p(z)\n\\]\nNotera att detta inte är samma sak som \\(p(y\\mid x)\\) om det finns confounders. För att kontrollera kausal effekt i notation:\n\nTa bort bakdörrar → kontrollera att:\n\n\\[\nY \\perp\\!\\!\\!\\perp X \\mid Z\n\\]\n\nTesta samma betingning på den ursprungliga grafen:\nOm sambandet inte är oberoende givet samma \\(Z\\):\n\n\\[\np(y \\mid z)\\, p(x \\mid z) \\neq p(y,x \\mid z)\n\\]\nDå är det just den kausala effekt vi vill skatta."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#mediation-och-front-door-passus",
    "href": "posts/DAG-del-2/index.html#mediation-och-front-door-passus",
    "title": "DAGs: En introduktion (del 2)",
    "section": "5. Mediation och front-door passus",
    "text": "5. Mediation och front-door passus\nVid mediation eller front-door justering blir framåtriktade vägar från \\(X\\) relevanta:\n\nOm vi vill dela upp effekten i direkt och indirekt måste vi betinga på mediatorer.\n\nVi måste också kontrollera confounders som påverkar både mediatorn \\(M\\) och utfallet \\(Y\\)."
  },
  {
    "objectID": "posts/DAG-del-2/index.html#matematisk-representation-av-dags-1",
    "href": "posts/DAG-del-2/index.html#matematisk-representation-av-dags-1",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Matematisk representation av DAGs",
    "text": "Matematisk representation av DAGs\nEn DAG kan alltid skrivas som en gemensam sannolikhetsfördelning med hjälp av kedjeregeln:\n\\[\nP(X_1, X_2, \\dots, X_n) = \\prod_{i} P(X_i \\mid \\text{Parents}(X_i))\n\\]\nVarje variabel beror alltså bara på sina föräldrar i grafen. Det är grunden till hur vi kan läsa av betingade oberoenden direkt ur grafen.\nI sannolikhetstermer betyder oberoende mellan två variabler \\(X\\) och \\(Y\\) givet \\(Z\\):\n\\[\nX \\perp\\!\\!\\!\\perp Y \\mid Z\n\\]\nvilket är ekvivalent med att:\n\\[\nP(Y,X \\mid Z) = P(Y \\mid X,Z)\\, P(X \\mid Z) = P(Y \\mid Z)\\, P(X \\mid Z)\n\\]\nAlltså: om \\(X \\perp\\!\\!\\!\\perp Y \\mid Z\\) gäller, påverkar inte \\(X\\) sannolikheten för \\(Y\\) när vi redan känner till \\(Z\\).\n\n\nFrån korrelation till kausalitet\nI vanliga modeller tittar vi på \\(P(Y \\mid X)\\) – sambandet mellan \\(Y\\) och \\(X\\) i observerade data.\nMen detta säger inget om hur världen skulle se ut om vi ändrade \\(X\\).\nFör det behöver vi istället:\n\\[\nP(Y \\mid do(X))\n\\]\ndär \\(do(X)\\) innebär att vi aktivt sätter \\(X\\) till ett värde och därmed bryter alla pilar in till \\(X\\). Skillnaden kan formuleras så här:\n\\[\nP(Y \\mid do(X)) \\neq P(Y \\mid X)\n\\]\neftersom den första beskriver ett interventionsscenario, medan den andra beskriver ett observerat samband.\n\n\n\nIdentifiering med bakdörrsjustering\nFör att skatta den totala kausala effekten av \\(X\\) på \\(Y\\):\n\nBlockera alla icke-kausala vägar som går in i \\(X\\) (bakdörrar) med en uppsättning variabler \\(Z\\).\n\nVägar som går ut från \\(X\\) (framåtriktade icke-kausala vägar) påverkar normalt inte den totala effekten, men blir relevanta vid mediation eller front-door justering.\n\nOm bakdörrar är korrekt blockerade gäller:\n\\[\nY \\perp\\!\\!\\!\\perp X \\mid Z \\quad \\Leftrightarrow \\quad p(y,x \\mid z) = p(y \\mid x,z)\\, p(x \\mid z)\n\\]\n\n\n\nFrån betingad sannolikhet till kausal effekt\nNär bakdörrar är blockerade kan vi skriva:\n\\[\np(y \\mid do(x)) = \\sum_z p(y \\mid x,z)\\, p(z)\n\\]\nNotera att detta inte är samma sak som \\(p(y\\mid x)\\) om det finns confounders. För att kontrollera kausal effekt i notation:\n\nTa bort bakdörrar → kontrollera att:\n\n\\[\nY \\perp\\!\\!\\!\\perp X \\mid Z\n\\]\n\nTesta samma betingning på den ursprungliga grafen:\nOm sambandet inte är oberoende givet samma \\(Z\\):\n\n\\[\np(y \\mid z)\\, p(x \\mid z) \\neq p(y,x \\mid z)\n\\]\nDå är det just den kausala effekt vi vill skatta.\n\n\n\nMediation och front-door passus\nVid mediation eller front-door justering blir framåtriktade vägar från \\(X\\) relevanta:\n\nOm vi vill dela upp effekten i direkt och indirekt måste vi betinga på mediatorer.\n\nVi måste också kontrollera confounders som påverkar både mediatorn \\(M\\) och utfallet \\(Y\\)."
  },
  {
    "objectID": "posts/DAG-del-1/index.html#dags-kan-också-fungera-som-ett-gemensamt-språk-och-kommunikationsverktyg-mellan-statistiker-forskare-ämnesexperter-och-andra-intressenter.",
    "href": "posts/DAG-del-1/index.html#dags-kan-också-fungera-som-ett-gemensamt-språk-och-kommunikationsverktyg-mellan-statistiker-forskare-ämnesexperter-och-andra-intressenter.",
    "title": "DAGs: En introduktion (del 1)",
    "section": "DAGs kan också fungera som ett gemensamt språk och kommunikationsverktyg mellan statistiker, forskare, ämnesexperter och andra intressenter.",
    "text": "DAGs kan också fungera som ett gemensamt språk och kommunikationsverktyg mellan statistiker, forskare, ämnesexperter och andra intressenter.\n\nVad som kommer härnäst\nI nästa del kommer jag ta upp:\n\nnågra mer komplexa, men verklighetsnära exempel,\n\nta upp vad DAGs inte visar\noch lite matematisk notation gällande DAG"
  },
  {
    "objectID": "posts/Kvalitetsdeklarera/index.html",
    "href": "posts/Kvalitetsdeklarera/index.html",
    "title": "DAGs: En introduktion (del 2)",
    "section": "",
    "text": "I första delen gick jag igenom de grundläggande byggstenarna i en DAG –\nhur vi kan representera orsakssamband med pilar, och hur olika vägar mellan variabler\nkan blockeras eller öppnas beroende på vilka vi betingar på, vilket kan leda till att kausala effekter kan skattas utan bias.\nDen här delen börjar med hur man kan resonera med DAGs i mer komplexa, verklighetsnära exempel. Sedan diskuterar jag vad DAGs inte kan visa – och avslutar med den matematiska grunden för hur man formellt kan uttrycka kausala antaganden."
  },
  {
    "objectID": "posts/Kvalitetsdeklarera/index.html#dags-i-mer-komplexa-situationer",
    "href": "posts/Kvalitetsdeklarera/index.html#dags-i-mer-komplexa-situationer",
    "title": "DAGs: En introduktion (del 2)",
    "section": "DAGs i mer komplexa situationer",
    "text": "DAGs i mer komplexa situationer\nFöljande tre exempel är DAGs som är en kombination av de grundläggande strukturerna vi pratade om i del 1. Dessa kommer innehålla abstrakta symboler då jag känner att det viktigaste är bara att illustrera hur man kan resonera kring öppna och stängda vägar, bias och flöden i mer komplexa situationer.\nExempel 1 och 2 kan illustrera fiktiva experiment - random control trial (RCT) - då vi i dessa exempel inte har något som påverkar treatment. Exempel 3 skulle kunna vara ett exempel på en observationsstudie där vi har saker i modellen som påverkar treatment.\n\nExempel 1\nI det här exemplet antar vi följande kausala struktur:\n\n\n\n\n\n\n\n\nFigure 1: …\n\n\n\n\n\nFrån figuren kan vi utläsa att treatment antas ha en kausal påverkan på outcome: en direkt och en indirekt effekt:\n\ndirekt effekt: \\(treatment \\rightarrow outcome\\)\nindirekt effekt: \\(treatment \\rightarrow A \\rightarrow outcome\\)\n\nVi kan också se att vi har en chain (\\(treatment \\rightarrow A \\rightarrow outcome\\)), en collider som inte innefattar \\(outcome\\) i mitten (\\(treatment \\rightarrow A \\leftarrow B\\)), en fork (\\(A \\leftarrow B \\rightarrow outcome\\)) och en effect modifier \\(C\\).\nSå här kan vi resonera för att förstå hur en eventuell kausal effekt kan skattas i det här fallet:\n\nOm vi är ute efter att skatta den genomsnittliga totala effekten:\n\nVi ska inte betinga på mediatorn A, av just det skälet att vi inte är intresserade av att dela upp den totala effekten i dess komponenter.\nDå vi inte har betingat på A så gör det varken till eller ifrån om vi betingar på B för inget flöde kan gå från treatment till outcome via B just för att vi inte betingat på A, vilket fungerar som en collider här.\nAtt betinga på C är inte nödvändigt för syftet.\n\nOm vi är intresserade av att dela upp den genomsnittliga totala effekten i direkt och indirekt effekt (mediation analys):\n\nVi ska betinga på A för att ens kunna skatta de olika effekterna.\nDå vi nu har betingat på A så måste vi betinga på B, annars kommer det fungera som en confounder mellan mediatorn A och outcome och bidra med ett skenbart, icke-kausalt samband.\nSom tidigare så är det inte nödvändigt för syftet att betinga på C.\n\n\n\n\nExempel 2\nI förra exemplet kom vi fram till att:\n\nom vi ska skatta den totala effekten får vi inte betinga på mediatorn A\nom vi vill skatta den direkta och indirekta effekten ska vi:\n\nbetinga på A och betinga på B\n\n\nHär vill jag bara illustrera vad det skulle resultera i om det i praktiken inte fanns några kausala väger mellan treatment och outcome som figuren nedan visar\n\n\n\n\n\n\n\n\nFigure 2: …\n\n\n\n\n\nom vi försökte skatta den totala effekten genom att inte betinga på något skulle vi göra rätt: vi skulle i genomsnitt skatta den totala effekten till 0.\nOm vi försökte skatta direkt och indirekt effekt genom att betinga på både A och B skulle vi också göra rätt. Här är det extra tydligt att om vi skulle betinga på mediatorn A så skulle vi öppna upp ett skenbart samband treatment och outcome via A och B.\n\n\nExempel 3\nI det här exemplet antar vi följande kausala struktur:\n\n\n\n\n\n\n\n\nFigure 3: …\n\n\n\n\n\nFrån figuren kan vi utläsa att treatment antas ha en kausal påverkan på outcome, en direkt och en indirekt effekt:\n\ndirekt effekt: \\(treatment \\rightarrow outcome\\)\nindirekt effekt: \\(treatment \\rightarrow C \\rightarrow outcome\\)\n\nVi kan också se att vi har en chain (\\(treatment \\rightarrow C \\rightarrow outcome\\)), ingen collider som inte innefattar \\(outcome\\) eller treatment i mitten en fork (\\(treatment \\leftarrow B \\rightarrow outcome\\)) och en effect modifier \\(A\\).\nSå här kan vi resonera för att förstå hur en eventuell kausal effekt kan skattas i det här fallet:\n\nOm vi är ute efter att skatta den genomsnittliga totala effekten:\n\nVi ska inte betinga på mediatorn C, av just det skälet att vi inte är intresserade av att dela upp den totala effekten i dess komponenter.\nför att inte sambandet mellan treatment och outcome ska innehålla ett skenbart samband måste vi betinga på B och stänga forken\nAtt betinga på A är inte nödvändigt för syftet.\n\nOm vi är intresserade av att dela upp den genomsnittliga totala effekten i direkt och indirekt effekt (mediation analys) är den enda skillnaden att vi ska betinga på C"
  },
  {
    "objectID": "posts/Kvalitetsdeklarera/index.html#vad-en-dag-inte-visar",
    "href": "posts/Kvalitetsdeklarera/index.html#vad-en-dag-inte-visar",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Vad en DAG inte visar",
    "text": "Vad en DAG inte visar\nEn DAG beskriver strukturella antaganden om kausala relationer,\nmen den säger ingenting om hur starka dessa relationer är, eller hur de ser ut matematiskt.\nDen visar alltså att något påverkar något – inte hur eller hur mycket.\nPilen \\(X \\rightarrow Y\\) betyder inte att effekten är linjär, konstant eller ens positiv.\nDen betyder bara att \\(X\\) påverkar \\(Y\\) på något sätt. Är det flera variabler som påverkar \\(Y\\) kan dessa interagera på ett komplext sätt.\nDet här är lätt att glömma när man arbetar med verkliga data –\noch något jag tycker det pratades om alldeles för lite i min utbildning. Jag har sett det här missförståndet dyka upp i forskning och praktiska tillämpningar:\natt man “tar hänsyn till” olika variabler genom att lägga till dem additivt i en linjär regressionsmodell, utan att reflektera över om modellen verkligen motsvarar den kausala struktur man tänker sig, både i termer av vad man har med och variablernas funktionella form.\nEn korrekt ritad DAG, där man tar hänsyn till de variabler man behöver, kan alltså ändå leda till fel slutsatser om man gör fel antaganden om den funktionella formen –\ndet vill säga hur \\(Y\\) faktiskt beror på \\(X\\) och andra variabler, exempelvis \\(Z\\) och \\(W\\):\n\\[\nY = f(X, Z, W)\n\\]\nTvå situationer kan ha exakt samma kausala struktur men helt olika form på sambandet.\nI den ena kanske sambandet är linjärt:\n\\[\nY = \\alpha + \\beta X + \\varepsilon\n\\]\nmedan det i den andra är icke-linjärt:\n\\[\nY = \\alpha + \\beta_1 X + \\beta_2 X^2 + \\varepsilon\n\\]\nDAGen skulle se identisk ut i båda fallen, men effekten av \\(X\\) är helt olika."
  },
  {
    "objectID": "posts/Kvalitetsdeklarera/index.html#matematisk-representation-av-dags",
    "href": "posts/Kvalitetsdeklarera/index.html#matematisk-representation-av-dags",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Matematisk representation av DAGs",
    "text": "Matematisk representation av DAGs\nEn DAG kan alltid skrivas som en gemensam sannolikhetsfördelning med hjälp av kedjeregeln:\n\\[\nP(X_1, X_2, \\dots, X_n) = \\prod_i P(X_i \\mid \\text{Parents}(X_i))\n\\]\nVarje variabel beror alltså bara på sina föräldrar i grafen.\n\nI vanlig statistisk analys studerar man ofta \\(P(Y \\mid X)\\) – alltså hur \\(Y\\) varierar med \\(X\\) i observerade data.\nMen det är bara ett beskrivande samband, inte ett orsakssamband. För att beskriva vad som skulle hända om vi faktiskt ändrade \\(X\\) behöver vi istället använda:\n\\[\nP(Y \\mid do(X))\n\\]\ndär \\(do(X)\\) innebär att vi aktivt sätter \\(X\\) till ett värde. Skillnaden blir:\n\\[\nP(Y \\mid do(X)) \\neq P(Y \\mid X)\n\\]\nDen första beskriver ett interventionsscenario, medan den andra beskriver ett observerat samband.\nDet här är kärnan i övergången från korrelation till kausalitet.\n\nFör att kunna identifiera en total kausal effekt måste vi justera för alla icke-kausala samband mellan \\(X\\) och \\(Y\\).\nDet gör vi genom den så kallade bakdörrsjusteringen.\nMålet är att hitta en uppsättning variabler \\(Z\\) som blockerar alla vägar som börjar med pilar in i \\(X\\).\nSådana vägar representerar confounding-effekter som kan skapa skenbara samband.\nNär dessa bakdörrar är blockerade – det vill säga, när vi brutit alla icke-kausala vägar in i \\(X\\) – gäller att \\(X\\) och \\(Y\\) är oberoende givet \\(Z\\) i en graf där vi har tagit bort alla pilar från \\(X\\).\nVi betecknar denna modifierade graf som \\(G_{\\overline{X}}\\).\n\\[\nY \\perp\\!\\!\\!\\perp X \\mid Z\n\\quad \\text{i grafen } G_{\\overline{X}}\n\\]\nvilket är, i en sådan graf, ekvivalent med:\n\\[\np(y,x \\mid z) = p(y \\mid z)\\, p(x \\mid z)\n\\]\nDet betyder att \\(X\\) och \\(Y\\) blir oberoende givet \\(Z\\) efter att vi stängt dörrarna bakåt – alltså i grafen \\(G_{\\overline{X}}\\) där orsakspilarna från \\(X\\) tagits bort.\nMen i den ursprungliga grafen (där vägar både in i och ut från \\(X\\) fortfarande finns) ska samma betingning inte göra dem oberoende – annars finns ingen kausal effekt att identifiera:\n\\[\np(y \\mid z)\\, p(x \\mid z) \\neq p(y,x \\mid z)\n\\]\n\nNär villkoret ovan är uppfyllt kan sambandet mellan \\(X\\) och \\(Y\\) uttryckas i observerbara sannolikheter genom bakdörrsformeln:\n\\[\np(y \\mid do(x)) = \\sum_z p(y \\mid x,z)\\, p(z)\n\\]\nDetta är en central identitetsformel i kausal inferens.\nDen visar att vi kan beräkna en kausal effekt utan att utföra ett faktiskt experiment –\nså länge vi har justerat för rätt uppsättning variabler \\(Z\\) som blockerar alla bakvägar.\n\nEn viktig distinktion är att vägar från \\(X\\) inte behöver blockeras för att identifiera den totala effekten – men de kan bli relevanta om vi vill dela upp effekten i en direkt och en indirekt del (mediation analysis),\neller om man försöker skatta den totala effekten i situationer där bakvägar inte går att blockera.\nDessa scenarier kräver ofta starkare antaganden, men grunden är densamma:\natt vi genom grafen kan avgöra vilka oberoenden som måste gälla för att identifiera en kausal effekt."
  },
  {
    "objectID": "posts/Kvalitetsdeklarera/index.html#avslutning-från-antagande-till-analys",
    "href": "posts/Kvalitetsdeklarera/index.html#avslutning-från-antagande-till-analys",
    "title": "DAGs: En introduktion (del 2)",
    "section": "Avslutning: från antagande till analys",
    "text": "Avslutning: från antagande till analys\nDAGs är kraftfulla eftersom de tvingar oss att synliggöra våra antaganden.\nDe hjälper oss att skilja mellan vad som är teoretiskt möjligt, statistiskt identifierbart, och empiriskt testbart.\nMen de ersätter inte analysen – de kompletterar den.\nDe visar strukturen av världen så som vi tror den ser ut, men det är fortfarande upp till oss att välja rätt modell, form och metod.\nEn DAG kan alltså inte tala om för dig exakt hur du ska skatta en effekt – men den kan tala om vilka effekter som går att skatta överhuvudtaget."
  },
  {
    "objectID": "posts/Mappstruktur/index.html",
    "href": "posts/Mappstruktur/index.html",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "",
    "text": "När jag började jobba som statistiker blev jag introducerad till Robert C. Martins bok Clean Code. Den, tillsammans med erfarenheten jag snabbt fick – att man som metodstatistiker ofta arbetar med systemutveckling utan att ha den formella utbildningen för det – väckte mitt intresse för området.\nHur arbetar man egentligen med kod för att hålla den förvaltningsbar, transparent, reproducerbar och begriplig? Hur minskar man personberoende och bygger robusta rutiner som gör arbetet mer njutbart och kontrollerbart?\nSom sagt är jag ingen systemutvecklare, och en snabb sökning visar hur enormt området är. Det finns en uppsjö av utvecklings- och arkitekturprinciper, var och en med sina ideal och begrepp. Ämnet är komplext, och jag gör inga anspråk på att vara expert.\nMen jag vill ändå skriva om det – om de principer jag själv funnit mest användbara och hur de kan tillämpas i praktiskt arbete som statistiker, analytiker eller data scientist. Den här bloggserien är till stor del till för min egen skull: genom att få saker kopplade till detta nedskrivet lär jag mig under tiden.\nJag kommer utgå mycket från Quality Assurance of Code for Analysis and Research som är skriven inom ONS, och jag kan inte nog rekommendera alla att läsa den."
  },
  {
    "objectID": "posts/Mappstruktur/index.html#varför-är-struktur-kring-kod-viktigt",
    "href": "posts/Mappstruktur/index.html#varför-är-struktur-kring-kod-viktigt",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "Varför är struktur kring kod viktigt?",
    "text": "Varför är struktur kring kod viktigt?\nI Clean Code tar författaren upp ett exempel där ett företag gjorde en populär app som med tiden blev seg och kraschade stup i ett – tills företaget till slut gick i konkurs. Anledningen var dålig kod: de hade skyndat ut appen utan att strukturera eller underhålla koden. Varje ny funktion lades ovanpå något redan skört, tills allt föll ihop.\nPoängen är att den marginella produktiviteten av dåligt strukturerad kod sjunker över tid. Ju längre man väntar med att reda upp, desto dyrare blir det – och till slut når man en punkt där det inte går att rädda.\nFörfattarna är samtidigt ödmjuka: alla har skrivit dålig kod. Ibland har man bråttom, ibland är man lat, ibland känns dokumentation bara tråkig. Och det gäller inte bara dataanalytiker, utan även professionella utvecklare.\nDet kan vara trösterikt – men också en påminnelse om att vi inom statistik och analys ofta ligger ännu längre ifrån den professionella mjukvaruvärlden. Därför är det extra viktigt att vi tar till oss de verktyg och principer som finns. Inom offentlig sektor är det inte bara en fråga om estetik, utan om reproducerbarhet, transparens, trovärdighet och effektivitet.\nOch det fina är att bra struktur inte bara gör koden mer robust – den gör arbetet roligare. Det är en helt annan känsla att öppna ett välorganiserat projekt än att tänka:\n&gt; “Åh herregud… vad är det här?”"
  },
  {
    "objectID": "posts/Mappstruktur/index.html#från-princip-till-praktik-så-här-lägger-jag-upp-statistikprojekt",
    "href": "posts/Mappstruktur/index.html#från-princip-till-praktik-så-här-lägger-jag-upp-statistikprojekt",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "Från princip till praktik: så här lägger jag upp statistikprojekt",
    "text": "Från princip till praktik: så här lägger jag upp statistikprojekt\nNär jag började tänka mer systematiskt kring kodarbete upptäckte jag att Duck Book faktiskt fungerar väldigt bra även för statistikproduktion – med några anpassningar. Här är hur jag brukar göra i praktiken.\n\n\n1. En tydlig mappstruktur från start\nJag försöker alltid hålla en konsekvent struktur där data, kod och resultat är tydligt åtskilda.\nDet gör det lättare att förstå flödet, reproducera analyser och undvika att skriva över data av misstag.\nmy_stat_project/\n  README.md\n  docs/\n  data/\n    raw/\n    processed/\n  scripts/\n  outputs/\n  tests/\n  config/\nJag separerar alltid rådata, bearbetade data och utdata.\nJag numrerar även skript (t.ex. 01_load_data.R, 02_clean_data.R) för att göra körordningen tydlig.\nDet här gör att projekten blir mycket lättare att förstå både för andra och för framtida mig.\n\n\n\n2. Kodpraxis som håller\nAtt skriva kod för statistikproduktion handlar inte bara om att få något att fungera, utan om att göra arbetet förvaltningsbart, tydligt och reproducerbart. Här är några principer jag försöker följa.\nSkriv modulärt\nVarje fil eller funktion ska göra en sak. På så sätt blir koden lättare att förstå, testa och återanvända. Om något projekt växer eller förändras blir det också enklare att justera en del utan att riskera att störa allt annat.\nNamnsättning\nTydliga namn är en av de viktigaste sakerna för läsbarhet och förvaltning. Det gäller inte bara funktioner, utan också program, dataset, mappar och variabler. Namn ska ge en tydlig bild av vad något är eller vad det gör. Om du öppnar ett projekt efter ett halvår ska du kunna förstå syftet med varje del utan att behöva gissa. Bra namnsättning minskar också behovet av kommentarer, eftersom koden i sig berättar sin historia.\nKommentarer\nKommentarer är viktiga, men de ska användas med eftertanke. Du ska inte kommentera bort saker för att dölja problem, och du ska inte skriva kommentarer som förklarar något som redan är uppenbart. Sträva efter att skriva kod som är tillräckligt tydlig för att inte behöva kommentarer, men när kommentarer behövs, ska de förklara varför något görs på ett särskilt sätt eller ge kontext som inte syns i koden. På så sätt blir projektet mer transparent både för dig själv och för andra.\nSeparera kod och parametrar\nHårdkodade värden gör projekt svårare att underhålla. Att ha parametrar spridda på flera ställen riskerar att du behöver göra samma ändring på flera ställen om något förändras. Genom att samla inställningar, filvägar och parametrar på ett ställe blir det mycket enklare att justera projektet, och allt blir mer förvaltningsbart. Denna separation sparar tid, minskar risken för fel och gör projektet mer robust över tid.\nTestning\nTestning är centralt för kvalitet. Det handlar inte bara om att kontrollera allt manuellt, utan att automatisera tester så långt det går. Det som inte går att testa automatiskt bör generera någon form av output som kan granskas utan att man behöver köra koden steg för steg. På så sätt kan du säkerställa att viktiga delar av arbetet alltid kontrolleras, samtidigt som du sparar tid och minskar risken för mänskliga misstag.\n\n\n3. Versionshantering och dokumentation\nAlla mina projekt ligger i Git (ofta på GitHub eller internt). Jag försöker alltid:\ncommitta ofta med tydliga meddelanden,\nskriva en README som beskriver syfte, beroenden och körinstruktioner,\ntagga versioner vid publicering.\nEn bra README ska räcka för att en kollega ska kunna klona projektet och köra det utan att behöva fråga något.\nEtt minimalt exempel på README-struktur:\n# Syfte\nDetta projekt tar fram statistik över X baserat på Y.\n\n# Struktur\ndata/ – innehåller rådata och bearbetade data  \nscripts/ – innehåller kod i körordning  \noutputs/ – innehåller resultat, tabeller och figurer  \n\n# Körning\nsource(\"scripts/01_load_data.R\")\nsource(\"scripts/02_clean_data.R\")\nsource(\"scripts/03_analysis.R\")\n\n\n4.Kvalitetssäkring och peer review\nSmå tester, automatiserade kontroller och kodgranskning är tre hörnstenar i mitt arbetssätt. Jag inspireras mycket av ONS Duck Book-kapitlen Testing code och Peer review .\nDet behöver inte vara avancerat.\n\n\n5.Från kod till reproducerbar produktion\nNär allt fungerar handlar det om att göra produktionen reproducerbar och robust.\nKör hela flödet via ett huvudskript (t.ex. run_all.R) eller Makefile.\nDokumentera versioner av data och kod.\nAutomatisera rapportgenerering, gärna med Quarto eller R Markdown.\nSpara körloggar och session info (sessionInfo()).\nMålet är att vem som helst – inklusive framtida du – ska kunna uppdatera siffrorna nästa år utan att behöva gissa hur något fungerade.\n\n\n6.Avslutning\nStruktur handlar inte om att vara pedant, utan om att frigöra tid för analys och kreativitet. Genom att arbeta medvetet med kodkvalitet och projektorganisation slipper man “spaghetti-kaoset” och får i stället en robust grund för framtida arbete.\nFör mig har vägen mot best practice blivit ett sätt att både växa i rollen som statistiker och att faktiskt tycka att kodning är roligare. Och om jag ska ge ett råd till mig själv (och andra): börja smått – men börja. En README, en mappstruktur, ett test i taget."
  },
  {
    "objectID": "posts/Mappstruktur/index.html#mappstruktur",
    "href": "posts/Mappstruktur/index.html#mappstruktur",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "Mappstruktur",
    "text": "Mappstruktur\nn genomtänkt mappstruktur är grunden för allt.\nDen gör att du och andra snabbt förstår var data ligger, var kod finns, och vad som är output.\nDet finns inget absolut rätt eller fel – det viktiga är att strukturen är konsekvent och begriplig. En bekant, välstrukturerad och tydlig struktur gör det lättare att navigera i både egna och andras projekt.\nNedanför är ett exempel på en grundläggande mappstruktur jag förordar för tillfället på root-nivå.\nprojekt_root/\n│\n├── README.md\n│\n├── config/\n│\n├── data/\n│\n├── scripts/\n│\n├── output/\n│\n├── docs/\n│\n├── backlog.md\n│\n└── ad_hoc/\n\nHur man tänker kring strukturen på root-nivå\nREADME.md\nEn kort beskrivning av projektet: syfte, hur man kör det, kontaktperson, och eventuella beroenden.\nMarkdown-formatet (.md) är smidigt – det ser ut som text, men tillåter rubriker, listor och länkar.\nconfig/\nHär ligger projektets konfigurationsfiler (config.yaml etc). De innehåller parametrar som styr körningen – exempelvis:\n\nvilket år som ska köras,\nrandom seeds,\nom temporära dataset ska sparas,\nroot-sökväg (om du kör på server),\neller vilka redovisningsgrupper som ska inkluderas.\n\nDet här undviker hårdkodning och gör projektet mer flexibelt och reproducerbart.\nI R kan du läsa in dessa med yaml::read_yaml() och kombinera med here::here() för att hantera sökvägar robust.\ndata/\nHär lagras data i tydliga underkategorier, till exempel:\ndata/\n│\n├── raw/\n│\n├── tmp/\n│\n└── processed/\nFör regelbundna körningar, t.ex. årliga körningar, kan man även lägga till årsmappar i data/ och output/, exempelvis:\ndata/\n│\n├── 2024/\n│   │\n│   ├── raw/\n│   │\n│   ├── tmp/\n│   │\n│   └── processed/\n│\n└── 2025/\n    │\n    ├── raw/\n    │\n    ├── tmp/\n    │\n    └── processed/\nscripts/\nHär ligger projektets huvudsakliga kod (pipelines).\nSkripten bör vara namnsatta enligt körordning, vilket underlättar att förstå flödet. Jag tycker man också ska samla funktioner i en undermapp. Funktionerna kan vara sånt som upprepas många gånger i den huvudsakliga koden eller en komplex bit i koden. De huvudsakliga syftena är att göra den huvudsakliga koden läsbar, förvaltningsbar och kvalitetssäker vid förändringsarbete. Att ändra på ett ställe i stället för 20 gör förändringsarbeten snabbare och säkrare. Ut över funktioner tycker jag man ska ha en undermapp för test. Här menar jag tester man har för att just testa koden och hitta fel i koden.\nSammanfattningsvist skulle detta göra att scripts/ exempelvis skulle kunna se ut såhär:\nscripts/\n│\n├── run_all.R\n│\n├── 00_import_raw_data.R\n│\n├── 01_load_data.R\n│\n├── 02_clean_data.R\n│\n├── 03_analysis.R\n│\n├── 04_figures.R\n│\n├── 06_reporting.R\n│\n├── 07_nonresponse_analysis.R\n│\n├── 08_delete_tmp.R\n│\n├── functions/\n│\n└── tests/\noutput/\n\nHär sparas resultat, figurer, tabeller och rapporter vilket gör att det skulle kunna se ut såhär:\n\noutput/\n│\n├── figures/\n│\n├── tables/\n│\n└── reports/\nFör återkommande produktioner kan man lägga till mappar direkt efter output/ i likhet med hur jag visade för data/.\ndocs/\nHär bör dokumentation kopplade till projektet ligga vilket skulle kunna vara: processbeskrivningar, funktionslistor, rapportmallar, flödesschema över produktionen, avtal osv.\n\noutput/\n│\n├── figures/\n│\n├── tables/\n│\n└── reports/\nbacklog.md\nEn lista med förbättringsförslag, idéer eller buggar.\nEn slags intern “to do”-lista som håller tankarna samlade över tid.\nad_hoc/\nSpecialanalyser eller tillfälliga körningar.\nJag brukar lägga varje ad hoc-projekt i en egen undermapp med samma struktur som huvudprojektet – det ger ordning utan att blanda in temporärt material i huvudflödet.\noutput/\n│\n├── figures/\n│\n├── tables/\n│\n└── reports/\nDet gör det tydligt vad som är input och output – och ger valbarhet mellan snabbhet och spårbarhet.\n\nDen här strukturen tycker jag är ren och tillräckligt flexibel mappstruktur som liknar de standardmallar man brukar se inom det här området."
  },
  {
    "objectID": "posts/Mappstruktur/index.html#projekt--och-kodflöde-pipeline",
    "href": "posts/Mappstruktur/index.html#projekt--och-kodflöde-pipeline",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "Projekt- och kodflöde (pipeline)",
    "text": "Projekt- och kodflöde (pipeline)\nEtt bra flöde är modulärt och reproducerbart. Med modulärt menas att man ska stycka upp kod i moduler som ska göra en tydlig sak var. Modulerna ska ha så lite beroende mellan varandra som möjligt, där man i praktiken ska kunna byta ut eller ändra en modul så länge man vet vad som är input och vad den förväntade outputen ska vara.\nSkripten ska, för reproducerbarhetens skull, kunna köras i följd via ett huvudskript, t.ex. run_all.R, vid en slutlig produktionskörning. Är processen beroende av kontroller så ska det helst automatiseras i koden och om det behövs bedömningar så ska det skickas ut output där man gör granskningar utanför skriptet.\nEtt typiskt flöde kan se ut så här:\n\nLäs in rådata om det inte redan finns\nSkapa nya mappar om det inte redan finns (exempelvis vid ny årlig körning)\nRensa, transformera, och spara bearbetade data.\nAnalysera\nGenerera tabeller, figurer, rapporter\nTa bort temporär data\n\nDå allt bör kunna köras från ett huvudskript, t.ex. run_all.R, tycker jag att man i slutet av varje skript ska spara ner outputen (som används som input i nästa steg) i data/tmp/, rensa den interna miljön på allt som skapats när skriptet körts och när man kör nästa del ska man läsa in den datan man behöver från exempelvis data/tmp/. Sist av allt ska man tömma dessa mellansteg, alltså allt i data/tmp/. Detta förfarande gör att man garanterar att det inte finns dolda beroenden mellan script, det tydliggör vad som krävs för att ett skripts ska kunna köras (input) och vad det förväntas generera (output) för att pipelinen ska fungera. Då reproducerbarhet är viktigt så tvingar detta förfarande oss att se till att flödet alltig genererar samma resultat från rådata till projektoutputs utan att vi är beroende av datat som ligger i data/tmp/. Dessa ska som sagt bara ses som tillfälliga och automatiska mellansteg."
  },
  {
    "objectID": "posts/Mappstruktur/index.html#kodskrivande",
    "href": "posts/Mappstruktur/index.html#kodskrivande",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "Kodskrivande",
    "text": "Kodskrivande\nHär kommer några principer jag tycker man fundera och applicera i sitt kodskrivande:\n1. Läsbar kod vinner ofta över effektiv kod\nDå vi ofta inte håller på med applikationer som ska kunna leverera realtidssnabba analyser eller funktionalitet så är ofta läsbarhet viktigare än effektiv kod. Det är klart att det finns en avvägning, men läsbarhet har ett stort värde: är det väldigt läsbart behövs ingen eller marginel dokumentation i koden vad som händer, personberoendet blir mindre då en som inte är insatt i programmen lättare kan förstå dem och förvaltningen blir lättare då en effektiv och komplex kod lätt kan bli svårare att förändra.\n2. Använd tydlig, konsekvent och smart namnsättning\nDetta gäller allt från variabelnamn till funktioner och datanamn. Namnet ska säga vad något är eller gör och helst var relativt självförklarande. clean_income_data() är till exempel bättre än func1(). Till detta så hör också att t.ex. använda datumstandarden YYYY-MM-DD, då detta gör sortering naturlig och effektiv. Här kan man också tänka på om man vill ha det som suffix eller prefix vilket också påverkar sorteringen, vad som är bäst beror på vad man vill. Är man enbart efter att bara sortera på när saker är genererat så är datumstandarden som prefix bäst då det kan skapa en sorterad lista som följer:\n2024-01-01_cleaned_data.csv\n2024-01-01_processed_data.csv\n...\n2025-01-01_cleaned_data.csv\n2025-01-01_processed_data.csv\nHar man det som suffix får man i stället en sorterad lista som först sorterar på namn (typ av data i detta fall) och inom respektive namn så sorteras det på datum:\ncleaned_data_2024-01-01.csv\ncleaned_data_2025-01-01.csv\n...\nprocessed_data_2024-01-01.csv\nprocessed_data_2025-01-01.csv\ncamelcase…\n…\n3. Använd kodhuvud \n….\n4. håll koden visuellt ren och strukturerad\n….\n\n5. Kommentera klokt\nKommentarer ska förklara varför något görs, inte vad.\nOm du behöver kommentera vad, är ofta koden för otydlig.\n6. Separera kod och parametrar\nHårdkodning försvårar underhåll. Lägg parametrar i config.yaml istället.\n5. Skala ner på kod genom att skapa funktioner\nSkapa ett litet funktionsbibliotek (scripts/functions/) för återkommande logik.\nDet kan på sikt paketeras som ett internt R-paket – så länge det är lätt att komma åt funktionerna via source() eller devtools::install() spelar det ingen roll var de bor.\n6. (bonus) Boy Scout Rule\nLämna alltid koden lite bättre än du fann den.\nDet behöver inte vara perfekt – men små förbättringar ackumuleras över tid."
  },
  {
    "objectID": "posts/Mappstruktur/index.html#dokumentation",
    "href": "posts/Mappstruktur/index.html#dokumentation",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "Dokumentation",
    "text": "Dokumentation\nDokumentation handlar inte om byråkrati – det handlar om att framtida du (eller någon annan) snabbt ska förstå vad som händer och vilka förändringar man gjort.\nJag brukar tänka på tre nivåer:\n\nProjekt-nivå\nREADME och docs/ med syfte, struktur och körinstruktioner.\nKod-nivå\nKommentarer, tydliga namn och gärna docstrings (med roxygen2 om du har funktioner).\nResultat-nivå\nKörloggar (logs/), sessionInfo(), versionsinformation för paket (renv::snapshot()).\n\nEtt exempel på enkel loggning:\n…"
  },
  {
    "objectID": "posts/Mappstruktur/index.html#slutligt-exempel-på-mappstruktur-med-innehåll",
    "href": "posts/Mappstruktur/index.html#slutligt-exempel-på-mappstruktur-med-innehåll",
    "title": "Från data till produkt: struktur i processen (del 1)",
    "section": "Slutligt exempel på mappstruktur med innehåll",
    "text": "Slutligt exempel på mappstruktur med innehåll\nStruktur handlar inte om att vara pedant – utan om att frigöra tid för analys och kreativitet.\nNär du slipper leta, tveka och felsöka i oreda, får du mer energi till det som faktiskt är roligt: att förstå, analysera och förklara data.\nOch som med det mesta: börja smått, men börja.\nEn tydlig README, en mappstruktur och ett litet funktionsbibliotek räcker långt – och ger dig ett försprång nästa gång du öppnar projektet och tänker:\n\n“Ah, just det – det här var faktiskt rätt snyggt gjort.”"
  },
  {
    "objectID": "posts/Lexi-diagram/index.html",
    "href": "posts/Lexi-diagram/index.html",
    "title": "Lexisdiagram: en introduktion",
    "section": "",
    "text": "1. Inledning\n\nSätt scenen: varför är tid, ålder och kohort viktiga dimensioner i statistik?\nKnyt till något bekant: “När vi analyserar befolkning, dödlighet eller utbildning tänker vi ofta i ålder eller år – men vad händer om vi ser båda samtidigt?”\nLyft huvudfrågan: hur kan vi strukturera och visualisera data där både ålder och kalenderår är relevanta?\n\n\nExempelmening:\n“Lexisdiagrammet är ett av de mest eleganta verktygen inom demografin – en enkel men kraftfull metod för att visa hur individer och händelser rör sig genom tid och ålder samtidigt.”\n\n\n\n2. Vad är ett Lexisdiagram?\n\nFörklara grunden: ett koordinatsystem där x-axeln är tid (kalenderår) och y-axeln är ålder.\nEn individ rör sig diagonalt uppåt med lutning 45° (ett år äldre per kalenderår).\nBeskriv skillnaden mellan:\n\n-    **Åldersperspektivet** (tvärsnitt)\n\n-    **Periodperspektivet** (år för år)\n\n-    **Kohortperspektivet** (följ en födelsekohort över tid)\n\nGärna en enkel illustration (du kan lägga till det i ett senare steg via Quarto eller ggplot).\n\n\n\n3. Varför är Lexisdiagrammet användbart?\n\nVisa dess tillämpningar:\n\n-    Befolkningsprognoser\n\n-    Dödlighetsanalyser (t.ex. livslängdsstudier)\n\n-    Epidemiologi (incidens, prevalens)\n\n-    Utbildnings- eller arbetslivsanalys\n\nDiskutera styrkan: ger överblick över relationen mellan ålder, period och kohort.\nKnyt till din egen erfarenhet: “När jag började använda Lexisdiagram insåg jag hur många missförstånd det löser…”\n\n\n\n4. Hur man läser ett Lexisdiagram\n\nFörklara de geometriska elementen:\n\n-    Horisontella linjer → samma ålder\n\n-    Vertikala linjer → samma kalenderår\n\n-    Diagonala linjer → samma födelseår (kohort)\n\nVisa hur en händelse (t.ex. död) kan placeras på rätt plats i diagrammet.\nFörklara skillnaden mellan “person-år” och “person” (vanlig fallgrop).\n\n\n\n5. Exempel i praktik\n\nVisa hur man kan skapa ett enkelt Lexisdiagram:\n\n-    Antingen med **ggplot2** (`geom_abline()`, `geom_tile()` etc.)\n\n-    Eller med färdiga paket som `LexisPlotR`, `Epi` eller `demography`.\n\nVisa exempeldata: t.ex. antal dödsfall efter ålder och år.\nTolka resultatet: visa hur ett Lexisdiagram hjälper att se kohorteffekter tydligt.\n\n\n\n6. Vanliga misstolkningar eller fallgropar\n\nFörväxling av period och kohort.\nFelaktiga tolkningar av diagonala trender.\nMissförstånd kring intervall: hur ett Lexis-rutnät används för att summera person-år korrekt.\n\n\n\n7. Lexisdiagram utanför demografin\n\nKort avsnitt: hur konceptet används i andra fält:\n\n-    Epidemiologi (incidens över tid och ålder)\n\n-    Arbetsmarknadsstatistik (anställning över ålder och år)\n\n-    Utbildning (examensålder, kohortjämförelser)\n\nVisa att principen “tid och ålder korsas” är generell.\n\n\n\n8. Reflektion / avslutning\n\nSammanfatta poängen: Lexisdiagrammet som tänkeverktyg, inte bara en figur.\nKnyt tillbaka till inledningen: “Att se statistik i Lexis-form hjälper oss förstå förändring, inte bara tillstånd.”\nEventuellt teaser för nästa inlägg: “I nästa del tänker jag visa hur man kan använda Lexisdiagram för att konstruera kohortmått.”"
  },
  {
    "objectID": "backlog.html",
    "href": "backlog.html",
    "title": "Blogginlägg",
    "section": "",
    "text": "Blogginlägg\n\nLexisdiagram"
  },
  {
    "objectID": "posts/Git (del 1)/index.html",
    "href": "posts/Git (del 1)/index.html",
    "title": "Git (del 1)",
    "section": "",
    "text": "Idén till denna bloggserie väcktes efter samtal med en tidigare kollega, som uppmärksammade att Git kan användas helt lokalt och som versionshantering av många typer av textbaserade filer, mer än bara kod. Serien är ett sätt för mig att strukturera och konkretisera ett enkelt arbetssätt för versionshantering i statistikproduktion, baserat på ett praktiskt utforskande av Git. Exemplen är förenklade och generiska."
  },
  {
    "objectID": "posts/Git (del 1)/index.html#varför-git-i-statistisk-produktion",
    "href": "posts/Git (del 1)/index.html#varför-git-i-statistisk-produktion",
    "title": "Git (del 1)",
    "section": "Varför Git i statistisk produktion?",
    "text": "Varför Git i statistisk produktion?\nÅterkommande statistikproduktion omfattar ofta databearbetning, framställning av tabeller, figurer och rapporter samt dokumentation. Det är mer regel än undantag att dataunderlag, definitioner, beräkningssteg och presentation förändras över tid, t.ex. mellan produktionsomgångar. Det kan bero på nya eller förändrade användarbehov eller vara en del av det kontinuerliga kvalitetsarbetet.\nDetta innebär att både kod och dokumentation utvecklas successivt. I praktiken ställer detta krav på ordning, spårbarhet och möjligheten att i efterhand återskapa hur ett visst resultat tagits fram.\n\nMindre manuell hantering av versioner\nI återkommande produktion uppstår naturligt behov av att hantera förändringar över tid. Ett arbetssätt är att spara tidigare versioner av kod och dokumentation i separata filer eller mappar, ofta med hjälp av namngivningskonventioner eller versionsmappar.\nSådana lösningar kan fungera, men innebär samtidigt ett behov av kontinuerlig disciplin och underhåll. Med tiden tenderar de också att skapa parallella strukturer för versionshistorik, dokumentation och arbetskopior.\nMed Git lagras förändringshistoriken istället som en integrerad del av arbetet. Det innebär bland annat att:\n\ntidigare versioner kan identifieras och återskapas utan att särskilda kopior sparas\nförändringar kan följas över tid\nkod och dokumentation kan versioneras tillsammans\n\nSyftet är inte att ersätta dokumentation utan att minska mängden dokumentation kopplat till förändringar och minska det tidskrävande manuella arbetet med att ha en arbetsrutin kring versionshistorik genom att kopiera kod, kopiera dokumentation, dokumentera förändringar utanför koden i sig osv.\n\n\nReproducerbarhet över flera produktionsomgångar\nReproducerbarhet i statistikproduktion innebär att det i efterhand ska vara möjligt att exakt återskapa hur ett visst resultat tagits fram, givet de förutsättningar som gällde vid tidpunkten för produktionen.\nDetta är särskilt relevant vid återkommande statistikproduktion där kod och dokumentation utvecklas och förändras allt eftersom. Utan systematisk versionshantering riskerar tidigare produktionsomgångar att bli svåra eller krångliga att återskapa, trots att kod och dokumentation i någon form finns kvar.\nGenom att versionera både kod och dokumentation kan ett tidigare tillstånd återställas exakt. Detta möjliggör reproduktion även när den aktuella kodbasen har utvecklats vidare långt efter den aktuella produktionstidpunkten av intresse.\n\n\nKod och dokumentation – separata men samordnade\nI statistikproduktion görs ofta en åtskillnad mellan koden och dokumentationen som beskriver metod, definitioner och antaganden.\nGenom att versionera dessa delar tillsammans kan förändringar i metodbeskrivning och implementation följas parallellt. Detta kan minska behovet av att överbelasta koden med omfattande kommentarer och i stället samla förklaringar i ett separat, versionerat dokument.\nEtt sådant upplägg kan bidra till att hålla koden mer fokuserad på implementation, samtidigt som dokumentationen förblir aktuell i relation till den version av koden som användes vid en viss produktion.\nMed Git blir det också möjligt att kunna se vem som gjort förändringar, när och varför (det sista förutsätter att personerna commitar med meddelanden). Att få all den informationen sparad i samband med att man gör förändringar, utan större ansträngning, kan vara guld värt om dokumentation brister, diciplinen inte alltid är på topp eller där många arbetar tillsammans med koden: antingen samtidigt i tid eller över tid.\n\n\nOberoende av verktyg och filformat\nDe principer som diskuteras här är oberoende av specifika verktyg eller programmeringsspråk. Versionshantering kan tillämpas på exempelvis:\n\ntextdokument\nkod (t.ex. R, SAS, SQL eller Python)\nrapportmallar\n\nDet centrala är inte filformatet, utan att produktionen organiseras på ett sätt som möjliggör spårbarhet och reproducerbarhet över tid.\n\n\nNästa inlägg\nI nästa inlägg visas grundläggande men tillräckliga Git-syntax för att använda Git som versionshantering."
  },
  {
    "objectID": "posts/Git (del 2)/index.html",
    "href": "posts/Git (del 2)/index.html",
    "title": "Git (del 2)",
    "section": "",
    "text": "I del 1 tog jag upp varför Git är användbart i statistikproduktion.\nI den här delen fokuserar vi på tillräcklig Git-syntax för att kunna arbeta tryggt och reproducerbart i en produktion där kod och dokumentation uppdateras över tid. Ambitionen är inte att täcka allt Git kan göra.\n\n\nTänk dig en situation där produktionen sker återkommande, t.ex. kvartalsvis, och att kod och dokumentation uppdateras mellan omgångar.\nI stället för att skapa separata filer eller versionsmappar för varje produktionsomgång används ett versionshanteringssystem för att:\n\nsamla all kod och dokumentation på ett ställe\nhålla reda på förändringar över tid\nmöjliggöra exakt reproduktion av tidigare produktionsomgångar\n\nKärnan i Git är:\n\nett repository (repo) - där all historik lagras\nen arbetskopia - där man faktiskt arbetar\n\n\n\nRepo är en förkortning för repository och är platsen där versionshistoriken sparas.\nOm produktionen man börjar arbeta med använder Git sedan innan så finns ofta ett repo redan, vilket förmodligen är det vanligaste.\nOftast använder man ett remote repository vilket är ett repo på en server. Exempel på det är Github och Gitlab. Men det kan även vara lokalt i en mapp.\n\nViktigt!\nSka man använda Git privat rekommenderar jag Github (eller lokalt1) för vilket man relativt enkelt kan Googla sig fram hur man ska gå till väga för att komma igång.\nSka man använda Git på arbetet så rekommenderar jag att man följer etablerat arbetssätt på arbetet gällande hur man skapar ett repo för Git! Be en kunnig kollega eller IT hjälpa dig sätta upp ett repo enligt arbetetsplatsens rutiner.\n\nVi antar att man kommer in i en produktion som redan använder Git och har ett remote repo på plats.\n\n\n\nBörja med att tala om för Git vem du är:\ngit config --global user.name \"[Ditt namn]\ngit config --global user.email \"[Din email]\"\nDenna information:\n\nföljer automatiskt med varje commit\när viktig dokumentation i produktion\ngör det lätt att se vem som gjort vad och när\n\nDetta behöver normailt bara göras en gång per dator.\n\n\n\nFör att se kopplingen i en arbetsmapp mot ett remote repo kan man köra git remote -v. Där kommer du se sökvägen eller repots URL.\n\n\nKlona repot:\ngit clone [väg till remote repo eller URL] [vad du vill att arbetsmappen ska heta]\nDetta:\n\nskapar en ny mapp\ninnehåller senaste versionen av allt\när redo att användas direkt\n\n\n\n\n\n.gitignore talar om för Git vad som inte ska versionshanteras. Det kan handla om känslig data eller tillfälliga filer.\nOm .gitignore saknas:\n\nSkapa en textfil som heter .gitignore\nLägg till den i repot:\n\ngit add .gitignore\ngit commit -m \"Lägger till .gitignore\"\ngit push\nSäg att man vill att följande saker ska ignoreras:\n\nmappen SkaIgnoreras/\nalla .Rdata-filer\n\nDå skulle .gitignore se ut så här:\nSkaIgnoreras/\n.Rdata\n\nViktigt!\nOm det berör känsligt innehåll och du precis initierat ett nytt .gitignore eller lagt till något nytt i .gitignore så tycker jag: testa att det funkar som det är tänkt med icke-känsligt innehåll.\n\n\n\n\nNär man gör ändringar i arbetsmappen behöver man tala om för Git vad som ska sparas.\n\nSe vad som ändrats med git status\nlägg till ändringar med git add \\[filnamn\\] eller allt på en gång med gid add .\nskapa en commit med git commit -m \"\\[kommentar\\]\" eller bara git commit\nskicka till remote repo med git push\n\nTänk på:\nEn commit är en dokumentationspunkt. Commita inte halvfärdiga ändringar, commita hellre en meningsfull ändring i taget med tillhörande kommentar som rör just den ändringen än att commita många ändringar med en generell kommentar.\n\n\n\nOm någon annan har arbetat i repot behöver du hämta deras ändringar vilket görs med git pull.\nDetta:\n\nhämtar ändringar från remote repo\nuppdaterar din arbetskopia\n\nRekommendation:\nkör git pull innan du börjar arbeta, särskilt i gemensamma projekt.\n\n\n\nOm något skulle saknas går det att ses med git status. För att få tillbaka det som saknas kan man köra git restore \\[filnamn\\] eller om allt saknas git restore ..\nEn sak som inte kan återskapas, med hjälp av Git, är det som nämns i .gitignore. Det kommer inte heller synas vid git status.\n\n\n\nGit gör det möjligt att se exakt hur projektet såg ut vid tidigare tillfällen.\n\n\n\nMed git log så kan man se historiken. Där ser man commit-id, vem som commitat, när och personens kommentar.\nFör att gå tillbaka till ett tidigare läge kan man köra:\ngit checkout &lt;commit-id&gt; \nDå återskapas alla filer vid det tillfället. Gör inga ändringar här utan använd det bara som ett read-only läge. Annars kan det uppkomma felmeddelanden när man ska ta sig tillbaka till nutiden.\n\n\n\nFör att ta sig tillbaka till nutiden kör man bara:\ngit checkout main \nAllt detta är särskilt användbart för:\n\nfelsökning\nreproducerbarhet\nförståelse av förändringar över tid\n\n\n\n\nDet här var min avslutande del om grunderna i Git. Vi får se om jag gör något mer om Git i framtiden.\nDet finns mycket mer man kan lära sig om Git, så som branch och stash, men med de sakerna jag har listat ovan kommer man långt med och räcker för en komplett versionshantering. Vissa av sakerna jag har tagit upp kanske till och med är lite överkurs.\nOavsett så tycker jag att Git är vägen att gå om man arbetar i en relativt komplex produktion där förändringar och tillägg tillkommer med en viss frekvens. En hel del av nödvändigt dokumentationsarbetet följer med per automatik, det går enkelt att återskapa produktionen vid tidigare skeenden och man minskar antalet rutiner när Git blir en naturlig del i produktionen!"
  },
  {
    "objectID": "posts/Git (del 2)/index.html#grundidén-ett-repo-och-en-arbetskopia",
    "href": "posts/Git (del 2)/index.html#grundidén-ett-repo-och-en-arbetskopia",
    "title": "Git (del 2)",
    "section": "",
    "text": "Tänk dig en situation där produktionen sker återkommande, t.ex. kvartalsvis, och att kod och dokumentation uppdateras mellan omgångar.\nI stället för att skapa separata filer eller versionsmappar för varje produktionsomgång används ett versionshanteringssystem för att:\n\nsamla all kod och dokumentation på ett ställe\nhålla reda på förändringar över tid\nmöjliggöra exakt reproduktion av tidigare produktionsomgångar\n\nKärnan i Git är:\n\nett repository (repo) - där all historik lagras\nen arbetskopia - där man faktiskt arbetar\n\n\n\nRepo är en förkortning för repository och är platsen där versionshistoriken sparas.\nOm produktionen man börjar arbeta med använder Git sedan innan så finns ofta ett repo redan, vilket förmodligen är det vanligaste.\nOftast använder man ett remote repository vilket är ett repo på en server. Exempel på det är Github och Gitlab. Men det kan även vara lokalt i en mapp.\n\nViktigt!\nSka man använda Git privat rekommenderar jag Github (eller lokalt1) för vilket man relativt enkelt kan Googla sig fram hur man ska gå till väga för att komma igång.\nSka man använda Git på arbetet så rekommenderar jag att man följer etablerat arbetssätt på arbetet gällande hur man skapar ett repo för Git! Be en kunnig kollega eller IT hjälpa dig sätta upp ett repo enligt arbetetsplatsens rutiner.\n\nVi antar att man kommer in i en produktion som redan använder Git och har ett remote repo på plats.\n\n\n\nBörja med att tala om för Git vem du är:\ngit config --global user.name \"[Ditt namn]\ngit config --global user.email \"[Din email]\"\nDenna information:\n\nföljer automatiskt med varje commit\när viktig dokumentation i produktion\ngör det lätt att se vem som gjort vad och när\n\nDetta behöver normailt bara göras en gång per dator.\n\n\n\nFör att se kopplingen i en arbetsmapp mot ett remote repo kan man köra git remote -v. Där kommer du se sökvägen eller repots URL.\n\n\nKlona repot:\ngit clone [väg till remote repo eller URL] [vad du vill att arbetsmappen ska heta]\nDetta:\n\nskapar en ny mapp\ninnehåller senaste versionen av allt\när redo att användas direkt\n\n\n\n\n\n.gitignore talar om för Git vad som inte ska versionshanteras. Det kan handla om känslig data eller tillfälliga filer.\nOm .gitignore saknas:\n\nSkapa en textfil som heter .gitignore\nLägg till den i repot:\n\ngit add .gitignore\ngit commit -m \"Lägger till .gitignore\"\ngit push\nSäg att man vill att följande saker ska ignoreras:\n\nmappen SkaIgnoreras/\nalla .Rdata-filer\n\nDå skulle .gitignore se ut så här:\nSkaIgnoreras/\n.Rdata\n\nViktigt!\nOm det berör känsligt innehåll och du precis initierat ett nytt .gitignore eller lagt till något nytt i .gitignore så tycker jag: testa att det funkar som det är tänkt med icke-känsligt innehåll.\n\n\n\n\nNär man gör ändringar i arbetsmappen behöver man tala om för Git vad som ska sparas.\n\nSe vad som ändrats med git status\nlägg till ändringar med git add \\[filnamn\\] eller allt på en gång med gid add .\nskapa en commit med git commit -m \"\\[kommentar\\]\" eller bara git commit\nskicka till remote repo med git push\n\nTänk på:\nEn commit är en dokumentationspunkt. Commita inte halvfärdiga ändringar, commita hellre en meningsfull ändring i taget med tillhörande kommentar som rör just den ändringen än att commita många ändringar med en generell kommentar.\n\n\n\nOm någon annan har arbetat i repot behöver du hämta deras ändringar vilket görs med git pull.\nDetta:\n\nhämtar ändringar från remote repo\nuppdaterar din arbetskopia\n\nRekommendation:\nkör git pull innan du börjar arbeta, särskilt i gemensamma projekt.\n\n\n\nOm något skulle saknas går det att ses med git status. För att få tillbaka det som saknas kan man köra git restore \\[filnamn\\] eller om allt saknas git restore ..\nEn sak som inte kan återskapas, med hjälp av Git, är det som nämns i .gitignore. Det kommer inte heller synas vid git status.\n\n\n\nGit gör det möjligt att se exakt hur projektet såg ut vid tidigare tillfällen.\n\n\n\nMed git log så kan man se historiken. Där ser man commit-id, vem som commitat, när och personens kommentar.\nFör att gå tillbaka till ett tidigare läge kan man köra:\ngit checkout &lt;commit-id&gt; \nDå återskapas alla filer vid det tillfället. Gör inga ändringar här utan använd det bara som ett read-only läge. Annars kan det uppkomma felmeddelanden när man ska ta sig tillbaka till nutiden.\n\n\n\nFör att ta sig tillbaka till nutiden kör man bara:\ngit checkout main \nAllt detta är särskilt användbart för:\n\nfelsökning\nreproducerbarhet\nförståelse av förändringar över tid\n\n\n\n\nDet här var min avslutande del om grunderna i Git. Vi får se om jag gör något mer om Git i framtiden.\nDet finns mycket mer man kan lära sig om Git, så som branch och stash, men med de sakerna jag har listat ovan kommer man långt med och räcker för en komplett versionshantering. Vissa av sakerna jag har tagit upp kanske till och med är lite överkurs.\nOavsett så tycker jag att Git är vägen att gå om man arbetar i en relativt komplex produktion där förändringar och tillägg tillkommer med en viss frekvens. En hel del av nödvändigt dokumentationsarbetet följer med per automatik, det går enkelt att återskapa produktionen vid tidigare skeenden och man minskar antalet rutiner när Git blir en naturlig del i produktionen!"
  },
  {
    "objectID": "posts/Git (del 2)/index.html#footnotes",
    "href": "posts/Git (del 2)/index.html#footnotes",
    "title": "Git (del 2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDet enklaste sättet att skapa ett lokalt repo är att köra git init --bare i den mapp du vill ha som ett “lokalt remote repo”. För att skapa en kopplad arbetsmapp kör man sen git clone [väg till repot] [vad du vill att arbetsmappen ska heta].↩︎"
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html",
    "href": "IntuitivStatistik/posts/Metadata/index.html",
    "title": "Metadata: före eller efter?",
    "section": "",
    "text": "Om du tänker att metadata är tidskrävande dokumentation eller ett grått tekniskt ämne så vill jag bara säga: det kan vara så – men det behöver inte vara så. Om man lägger upp arbetet rätt kan metadata till och med minska den totala dokumentationen, genom att reducera dubbelarbete och i slutändan också göra att produktionen följer dokumentationen snarare än tvärtom.\nTidigare tänkte jag inte särskilt mycket på metadata, utan var mer intresserad av analysarbete och de statistiska frågorna. Metadata kändes mest som en teknisk arkitekturfråga där värdet inte var helt uppenbart utöver att det hör till professionen.\nEfter att ha arbetat med statistikproduktion har jag insett att metadata har ett stort värde – och hur centralt arbetssättet kring metadata faktiskt är. Det stora värdet ligger i att metadata beskriver vad variabler egentligen betyder, exempelvis:\n\nvar de kommer ifrån\nvilken värdemängd de har\ni vilken skala de är mätta\nom de är härledda, och i så fall hur och från vilka variabler\nvilka variabler som är beroende av den specifika variabeln\n\nDet här är viktigt för användare av data, men också för produktionen i stort. Väl dokumenterad metadata minskar personberoende, gör förändringsarbete betydligt enklare och hör till professionell datahantering.\nSlutformatet kan till exempel vara en metadatakatalog i något system för enklare samkörning och säker lagring. Det betyder dock fortfarande att man antingen matar in saker manuellt eller gör någon form av mer eller mindre smart inläsning.\nHär tänkte jag resonera kring hur man kan arbeta med metadata: efter produktion eller före produktion."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#inledning-varför-metadata-ofta-blir-något-vid-sidan-av",
    "href": "IntuitivStatistik/posts/Metadata/index.html#inledning-varför-metadata-ofta-blir-något-vid-sidan-av",
    "title": "Metadata: före eller efter?",
    "section": "",
    "text": "Om du tänker att metadata är tidskrävande dokumentation eller ett grått tekniskt ämne så vill jag bara säga: det kan vara så – men det behöver inte vara så. Om man lägger upp arbetet rätt kan metadata till och med minska den totala dokumentationen, genom att reducera dubbelarbete och i slutändan också göra att produktionen följer dokumentationen snarare än tvärtom.\nTidigare tänkte jag inte särskilt mycket på metadata, utan var mer intresserad av analysarbete och de statistiska frågorna. Metadata kändes mest som en teknisk arkitekturfråga där värdet inte var helt uppenbart utöver att det hör till professionen.\nEfter att ha arbetat med statistikproduktion har jag insett att metadata har ett stort värde – och hur centralt arbetssättet kring metadata faktiskt är. Det stora värdet ligger i att metadata beskriver vad variabler egentligen betyder, exempelvis:\n\nvar de kommer ifrån\nvilken värdemängd de har\ni vilken skala de är mätta\nom de är härledda, och i så fall hur och från vilka variabler\nvilka variabler som är beroende av den specifika variabeln\n\nDet här är viktigt för användare av data, men också för produktionen i stort. Väl dokumenterad metadata minskar personberoende, gör förändringsarbete betydligt enklare och hör till professionell datahantering.\nSlutformatet kan till exempel vara en metadatakatalog i något system för enklare samkörning och säker lagring. Det betyder dock fortfarande att man antingen matar in saker manuellt eller gör någon form av mer eller mindre smart inläsning.\nHär tänkte jag resonera kring hur man kan arbeta med metadata: efter produktion eller före produktion."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#nivå-1-metadokumentation-i-efterhand",
    "href": "IntuitivStatistik/posts/Metadata/index.html#nivå-1-metadokumentation-i-efterhand",
    "title": "Tre nivåer av metadata - från dokumentation till styrning",
    "section": "Nivå 1 – Metadokumentation i efterhand",
    "text": "Nivå 1 – Metadokumentation i efterhand\nDen här nivån är kanske det mest intuitiva och kräver minst planering: man läser in data, bearbetar data, sparar data och sist dokumenterar data. Detta kan illustreras enligt flödesschemat nedan.\n\n\n\n\n\nflowchart LR\n  A[Datainläsning och databearbetning] --&gt;\n  id1[(Spara data)] --&gt; B[Metadatadokumnetation]\n\n\n\n\n\n\nI den enklaste formen skulle all metadata skrivas efteråt för alla variabler, kanske i stil med något YAML-liknande nedan:\n\n\n\n\n\nflowchart LR\nA[\n**Variabel 1**\n&Tab;gjgjg\n&Tab;&Tab;huhkj\n**Variabel 2**\n]\nstyle A text-align:left\nstyle A white-space: pre; \n\n\n\n\n\n\n\n\nMetadata nämns ofta som viktigt\n\nI praktiken hamnar den ofta:\n\n\ni separata dokument\n\ni excelfiler\n\ni wikis\n\n\n\nResultatet blir:\n\n\ndubbeldokumentation\n\nlåg tillit till metadata\n\nlåg koppling mellan dokumentation och faktisk produktion\n\n\n\n\nTes:\nDet är inte formatet på metadata som är avgörande, utan vilken roll metadata har i arkitekturen."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#nivå-1-metadokumentation-i-efterhand-1",
    "href": "IntuitivStatistik/posts/Metadata/index.html#nivå-1-metadokumentation-i-efterhand-1",
    "title": "Tre nivåer av metadata - från dokumentation till styrning",
    "section": "Nivå 1 – Metadokumentation i efterhand",
    "text": "Nivå 1 – Metadokumentation i efterhand\n”Vi beskriver det vi redan gjort”\n\nKaraktäristik\n\n\nMetadata skapas efter produktion\n\nAnvänds främst för:\n\n\nbeskrivningar\n\nkvalitetsdeklarationer\n\nvariabelbeskrivningar\n\n\n\nProduktionslogik finns bara i kod\n\n\n\n\nStyrkor\n\n\nLåg tröskel\n\nGer struktur och transparens\n\nFörbättrar kommunikation med användare\n\n\n\n\nBegränsningar\n\n\nMetadata kan bli inaktuell\n\nKräver parallellt underhåll\n\nPåverkar inte hur produktionen faktiskt körs\n\n\n\n\nTypiska symptom\n\n\n”Dokumentationen stämmer inte med verkligheten”\n\nMetadata uppdateras sist (eller aldrig)"
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#nivå-2-metadata-som-nyckel-för-automation",
    "href": "IntuitivStatistik/posts/Metadata/index.html#nivå-2-metadata-som-nyckel-för-automation",
    "title": "Tre nivåer av metadata - från dokumentation till styrning",
    "section": "Nivå 2 – Metadata som nyckel för automation",
    "text": "Nivå 2 – Metadata som nyckel för automation\n”Vi använder metadata för att slippa repetitiv kod”\n\nKaraktäristik\n\n\nMetadata innehåller nycklar, typer, roller, regler\n\nAnvänds för:\n\n\nkodgenerering\n\nvalidering\n\nkonfigurerbar logik\n\n\n\nProduktionen är delvis generell\n\n\n\n\nStyrkor\n\n\nMindre dubbeldokumentation\n\nMindre boilerplate-kod\n\nEnklare förändringar\n\nMetadata börjar bli en teknisk tillgång, inte bara dokumentation\n\n\n\n\nBegränsningar\n\n\nMycket logik ligger fortfarande hårdkodat\n\nOlika pipelines tolkar metadata olika\n\nArkitekturen är ofta inkonsekvent\n\n\n\n\nTypiska symptom\n\n\n“Vi har metadata – men ändå mycket specialfall i koden”\n\nAutomation finns, men är fragmenterad"
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#nivå-3-metadatastyrd-produktion",
    "href": "IntuitivStatistik/posts/Metadata/index.html#nivå-3-metadatastyrd-produktion",
    "title": "Metadata: före eller efter?",
    "section": "Nivå 3 – Metadatastyrd produktion",
    "text": "Nivå 3 – Metadatastyrd produktion\n”Metadata definierar vad systemet gör”\n\nKaraktäristik\n\n\nMetadata styr:\n\n\ndatainhämtning\n\nhärledningar\n\nvalideringar\n\npubliceringslogik\n\n\n\nProduktionssystemet är en generisk exekveringsmotor\n\nNya produkter skapas främst genom metadataändringar\n\n\n\n\nStyrkor\n\n\nMinimal dubbeldokumentation\n\nTydlig separation mellan:\n\n\nstyrning (metadata)\n\nexekvering (motor)\n\n\n\nHög förändringstakt utan kodändringar\n\nBättre spårbarhet mellan:\n\n\ndokumentation\n\nberäkning\n\npublicerat resultat\n\n\n\n\n\n\nUtmaningar\n\n\nHögre initial arkitekturkostnad\n\nKräver tydliga konventioner\n\nKräver disciplin i modellering\n\nMetadata blir affärskritisk infrastruktur"
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#jämförelse-vad-skiljer-nivåerna-åt",
    "href": "IntuitivStatistik/posts/Metadata/index.html#jämförelse-vad-skiljer-nivåerna-åt",
    "title": "Metadata: före eller efter?",
    "section": "Jämförelse: vad skiljer nivåerna åt?",
    "text": "Jämförelse: vad skiljer nivåerna åt?\n\n\n\nDimension\nNivå 1\nNivå 2\nNivå 3\n\n\n\n\nMetadata används för\nBeskrivning\nAutomation\nStyrning\n\n\nDubbeldokumentation\nHög\nMedel\nLåg\n\n\nKodändringar vid ändring\nOfta\nIbland\nSällan\n\n\nArkitekturell tyngd\nLåg\nMedel\nHög\n\n\nFörändringstakt\nLångsam\nMedel\nHög"
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#varför-detta-minskar-dubbeldokumentation",
    "href": "IntuitivStatistik/posts/Metadata/index.html#varför-detta-minskar-dubbeldokumentation",
    "title": "Metadata: före eller efter?",
    "section": "Varför detta minskar dubbeldokumentation",
    "text": "Varför detta minskar dubbeldokumentation\nNyckelpoängen:\n\nJu mer produktionen är beroende av metadata för att fungera,\ndesto mindre risk att metadata och verklighet divergerar.\n\nNär metadata:\n\n\nstyr beräkningar\n\nstyr validering\n\nstyr publicering\n\n\n…blir det tekniskt dyrt att ha fel metadata.\nDet är exakt den friktionen man vill åt."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#avslutning-metadata-som-arkitektur-inte-bilaga",
    "href": "IntuitivStatistik/posts/Metadata/index.html#avslutning-metadata-som-arkitektur-inte-bilaga",
    "title": "Metadata: före eller efter?",
    "section": "Avslutning: metadata som arkitektur, inte bilaga",
    "text": "Avslutning: metadata som arkitektur, inte bilaga\nSammanfattning:\n\n\nMetadata är inte binärt (“har/har inte”)\n\nDet finns mognadsnivåer\n\nDen stora skillnaden ligger i:\n\n\nom metadata beskriver systemet\n\neller om metadata är systemets styrning\n\n\n\n\nPraktisk reflektion:\nDe flesta organisationer befinner sig på nivå 1 eller 2 –\nmen pratar ofta som om de vore på nivå 3."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#nivå-1-metadatadokumentation-i-efterhand",
    "href": "IntuitivStatistik/posts/Metadata/index.html#nivå-1-metadatadokumentation-i-efterhand",
    "title": "Tre nivåer av metadata - från dokumentation till styrning",
    "section": "Nivå 1 – Metadatadokumentation i efterhand",
    "text": "Nivå 1 – Metadatadokumentation i efterhand\nDen här nivån är kanske den som kräver minst planering: vi har data som input, datan bearbetas, outputen sparas och sist dokumenterar vi den sparade outputdatan. Detta kan illustreras enligt flödesschemat nedan.\n\n\n\n\n\nflowchart LR\n  A[Input] --&gt; C[Databearbetning] --&gt;\n  id1[(Output)] -.-&gt; B[Metadatadokumnetation]\n\n\n\n\n\n\nI den enklaste formen skulle all metadata skrivas efteråt för alla variabler, kanske i stil med något YAML-liknande nedan:\n\n\n\n\n\nflowchart LR\nA[\n**Variabel 1**\n&Tab;Beskrivning: Utbildningsnivå\n&Tab;Typ: chr\n&Tab;Värdemängd:\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Låg\n&Tab;&Tab;&Tab;2: Medel\n&Tab;&Tab;&Tab;3: Hög\n&Tab;&Tab;&Tab;99: Okänt\n&Tab;Ursprung:\n&Tab;&Tab;Källa: Register\n&Tab;&Tab;&Tab;Register: Register XYZ\n&Tab;&Tab;&Tab;Registerversion: ÅÅÅ-MM-DD\n&Tab;#91;osv...#93;\n\n**Variabel 56**\n&Tab;Beskrivning: Mått på ...\n&Tab;Typ: int\n&Tab;Värdemängd:\n&Tab;&Tab;Min:\n&Tab;&Tab;&Tab;Minvärde: 0\n&Tab;&Tab;&Tab;Inklusivt: TRUE\n&Tab;&Tab;Max:\n&Tab;&Tab;&Tab;Maxvärde: FALSE\n&Tab;&Tab;&Tab;Inklusivt: NA\n&Tab;&Tab;Specialvärden:\n&Tab;&Tab;&Tab;-999: Partiellt bortfall\n&Tab;Ursprung:\n&Tab;&Tab;Källa: Härledd\n&Tab;&Tab;&Tab;Beroenda av: #91;Variabel 13, Variabel 16, ...#93; \n&Tab;&Tab;&Tab;Hur: func#40; #91;Variabel 13, Variabel 16, ...#93; #41;\n&Tab;#91;osv...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \n\n\n\n\n\n\nVisst, saker och ting kanske inte ändras så mycket, så det kan funka bra och vara mer än tillräckligt. I praktiken och om saker och ting ändras så skulle man kunna tänka sig att man gör dokumentationen bredvid eventuell omkodning i databearbetning. Det skulle göra saker lite mer effektivt men det skulle ändå kräva ytterligare ett lager av struktur, disciplin och i praktiken skulle man göra samma saker två gånger direkt efter varandra.\nLever man inte upp till det arbetet så finns risk att man har en dokumentation som skiljer sig från faktisk produktion och en dokumentation med låg tillit."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#nivå-2-nyckelbaserad-metadatadokumentation-i-efterhand",
    "href": "IntuitivStatistik/posts/Metadata/index.html#nivå-2-nyckelbaserad-metadatadokumentation-i-efterhand",
    "title": "Tre nivåer av metadata - från dokumentation till styrning",
    "section": "Nivå 2 – Nyckelbaserad metadatadokumentation i efterhand",
    "text": "Nivå 2 – Nyckelbaserad metadatadokumentation i efterhand\nDen här nivån är som förra nivån: en dokumentation man gör i efterhand eller parallelt med produktionsförändringar. Men till skillnad från förut gör man det effektivare genom att utnyttja att vissa saker återkommer om och om igen och däför kan vi använda “nycklar”."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#nivå-3---metadatastyrd-datahantering",
    "href": "IntuitivStatistik/posts/Metadata/index.html#nivå-3---metadatastyrd-datahantering",
    "title": "Metadata: före eller efter?",
    "section": "Nivå 3 - Metadatastyrd datahantering",
    "text": "Nivå 3 - Metadatastyrd datahantering\nDen här nivån ”Vi använder metadata för att slippa repetitiv kod”\n\nKaraktäristik\n\n\nMetadata innehåller nycklar, typer, roller, regler\n\nAnvänds för:\n\n\nkodgenerering\n\nvalidering\n\nkonfigurerbar logik\n\n\n\nProduktionen är delvis generell\n\n\n\n\nStyrkor\n\n\nMindre dubbeldokumentation\n\nMindre boilerplate-kod\n\nEnklare förändringar\n\nMetadata börjar bli en teknisk tillgång, inte bara dokumentation\n\n\n\n\nBegränsningar\n\n\nMycket logik ligger fortfarande hårdkodat\n\nOlika pipelines tolkar metadata olika\n\nArkitekturen är ofta inkonsekvent\n\n\n\n\nTypiska symptom\n\n\n“Vi har metadata – men ändå mycket specialfall i koden”\n\nAutomation finns, men är fragmenterad"
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#metadatadokumentation-i-efterhand",
    "href": "IntuitivStatistik/posts/Metadata/index.html#metadatadokumentation-i-efterhand",
    "title": "Metadata: före eller efter?",
    "section": "Metadatadokumentation i efterhand",
    "text": "Metadatadokumentation i efterhand\nKort sagt: Dokumentation i efterhand är enkelt att börja med – men riskerar att släpa efter över tid och kräver dubbelarbete.\nAtt dokumentera metadata i efterhand kräver minst planering: vi har data som input, datan bearbetas, outputen sparas – och sist dokumenterar vi den sparade outputdatan. Detta kan illustreras enligt flödesschemat nedan.\n\n\n\n\n\nflowchart LR\n  A[Input] --&gt; C[Databearbetning] --&gt;\n  id1[(Output)] -.-&gt; B[Metadatadokumnetation]\n\n\n\n\n\n\nI den enklaste formen skulle all metadata skrivas efteråt för alla variabler, kanske i stil med något liknande nedan:\n\n\n\n\n\nflowchart LR\nA[\n**Variabel 1**\n&Tab;Beskrivning: Utbildningsnivå\n&Tab;Typ: chr\n&Tab;Värdemängd:\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Låg\n&Tab;&Tab;&Tab;2: Medel\n&Tab;&Tab;&Tab;3: Hög\n&Tab;&Tab;&Tab;99: Okänt\n&Tab;Ursprung:\n&Tab;&Tab;Källa: Register\n&Tab;&Tab;&Tab;Register: Register XYZ\n&Tab;&Tab;&Tab;Registerversion: ÅÅÅ-MM-DD\n&Tab;#91;...#93;\n\n**Variabel 56**\n&Tab;Beskrivning: Mått på ...\n&Tab;Typ: int\n&Tab;Värdemängd:\n&Tab;&Tab;Min:\n&Tab;&Tab;&Tab;Minvärde: 0\n&Tab;&Tab;&Tab;Inklusivt: TRUE\n&Tab;&Tab;Max:\n&Tab;&Tab;&Tab;Maxvärde: FALSE\n&Tab;&Tab;&Tab;Inklusivt: NA\n&Tab;&Tab;Specialvärden:\n&Tab;&Tab;&Tab;-999: Partiellt bortfall\n&Tab;Ursprung:\n&Tab;&Tab;Källa: Härledd\n&Tab;&Tab;&Tab;Beroenda av: #91;Variabel 13, Variabel 16, ...#93; \n&Tab;&Tab;&Tab;Hur: func#40; #91;Variabel 13, Variabel 16, ...#93; #41;\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \n\n\n\n\n\n\nI vissa fall kan detta fungera helt okej – om produktion är av engångs- eller ad hoc-karaktär eller om saker inte förändras så mycket över tid. I praktiken brukar dock definitioner, kodning och härledningar förändras. Då kan man försöka dokumentera parallellt med omkodning i databearbetningen vilket är definitionen av dubbelarbete.\nDet är något effektivare, men innebär fortfarande ett extra lager av struktur och disciplin – och i praktiken gör man ofta samma sak två gånger. När detta arbetssätt inte upprätthålls finns risken att dokumentationen gradvis glider isär från den faktiska produktionen, vilket leder till lägre tillit till metadatat.\n\n\nEffektivisering med nycklar\nOavsett om man dokumenterar metadata före eller efter produktion är ett effektivt arbetssätt att använda nycklar.\nMed nycklar menar jag standardiserade koder som – när man kopplar ihop dokument – expanderar till mer detaljerade beskrivningar. I surveystatistik har man ofta samma svarsalternativ för många frågor. Istället för att upprepa samma värdemängd om och om igen kan man använda en gemensam kod som pekar på en central definition. Det minskar duplicering och gör ändringar i värdemängder potentiellt mer spårbara och kontrollerbara.\n\n\n\n\n\nflowchart LR\nA[\n*Variabel 1*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;#91;...#93;\n*Variabel 43*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \nB[\n*Värdemängdstabell*\n**Kod: JaNej**\n&Tab;Typ: chr\n&Tab;Värdemängd:\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Ja\n&Tab;&Tab;&Tab;2: Nej\n&Tab;&Tab;&Tab;3: Vet Ej\n&Tab;&Tab;&Tab;4: Vill inte svara\n&Tab;&Tab;&Tab;99: Partiellt bortfall\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \nC[\n*Variabel 1*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;&Tab;Typ: chr\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Ja\n&Tab;&Tab;&Tab;2: Nej\n&Tab;&Tab;&Tab;3: Vet Ej\n&Tab;&Tab;&Tab;4: Vill inte svara\n&Tab;&Tab;&Tab;99: Partiellt bortfall\n&Tab;#91;...#93;\n*Variabel 43*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;&Tab;Typ: chr\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Ja\n&Tab;&Tab;&Tab;2: Nej\n&Tab;&Tab;&Tab;3: Vet Ej\n&Tab;&Tab;&Tab;4: Vill inte svara\n&Tab;&Tab;&Tab;99: Partiellt bortfall\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \nA --&gt; C\nstyle B text-align:left\nstyle B white-space: pre; \nB --&gt; C\nstyle C text-align:left\nstyle C white-space: pre; \n\n\n\n\n\n\nHar man mycket återkommande struktur lönar det sig nästan alltid att normalisera metadatat i separata tabeller. Har man metadata i en nestlad struktur kan man också enkelt plocka ut delmängder vid behov – till exempel för att generera en kodbok där variabelnamn, beskrivning och värdemängd ofta räcker långt."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#metadatadokumentation-innan-data",
    "href": "IntuitivStatistik/posts/Metadata/index.html#metadatadokumentation-innan-data",
    "title": "Metadata: före eller efter?",
    "section": "Metadatadokumentation innan data",
    "text": "Metadatadokumentation innan data\nKort sagt: Metadatastyrd produktion kräver mer initialt arbete – men betalar tillbaka sig i förvaltning och kvalitet.\nAtt dokumentera metadata innan data kan låta märkligt, men vid återkommande produktion är det ofta både smart, effektivt, förvaltningsbart och kvalitetssäkert: om man planerar för det!\nTanken är att göra produktionen metadatastyrd: koden byggs upp så att den läser in både data och metadata och bearbetar datan utifrån metadata.\nSchematiskt:\n\n\n\n\n\nflowchart LR\n  A[Input] --&gt; C[Databearbetning] --&gt;\n  id1[(Output)]\n  B[Metadatadokumnetation] --&gt; C\n\n\n\n\n\n\nFördelarna är tydliga:\n\nDokumentationen blir primär, inte sekundär.\nMetadatat kan inte släpa efter – produktionen kräver korrekt metadata.\nDubbelarbete försvinner.\nKoden blir generisk: istället för speciallogik per variabel räcker en funktion per moment.\nFörvaltningen blir enklare.\n\nMed ett sådant upplägg kan man till exempel:\n\nKontrollera indata genom villkor i metadatat.\nHärleda nya variabler genom att specificera beroenden och transformationsregler i metadata.\nStyra vilka variabler som ska användas för redovisning och skattning baserat på metadata.\nGöra systematiska slutkontroller av färdig data.\nDetta är bara några exempel – det finns mycket mer man kan göra med ett metadatadrivet angreppssätt.\n\nNackdelen är primärt att det kräver planering men också att funktioner som är generella tenderar att bli mindre läsvänliga och kan bli komplexa när bearbetning har beroende i många steg."
  },
  {
    "objectID": "IntuitivStatistik/posts/Metadata/index.html#avslutning-metadata-som-arbetssätt-inte-bihang",
    "href": "IntuitivStatistik/posts/Metadata/index.html#avslutning-metadata-som-arbetssätt-inte-bihang",
    "title": "Metadata: före eller efter?",
    "section": "Avslutning: metadata som arbetssätt, inte bihang",
    "text": "Avslutning: metadata som arbetssätt, inte bihang\nJag tycker det är intressant att se att metadata kan vara en del av själva produktionslogiken. Att dynamiken i en produktion kan förändras beroende på metadatans roll: från ett dokumentationssteg till styrande och själva hjärtat i flödet.\nDet betyder inte att metadatastyrda arbetssätt alltid är rätt väg. I mindre, engångsbetonade projekt är tröskeln ofta för hög i relation till nyttan. Men i återkommande produktion, där förändringar sker över tid och där flera personer är inblandade kan det vara värt att åtminstonde fundera på metadatans roll.\nPoängen är inte att bygga det perfekta metadatasystemet, utan att vara medveten om vilken roll metadata har och kan ha. Om metadata ses som ett nödvändigt ont kommer den sannolikt alltid att ligga efter. Om den ses som ett verktyg för att hantera kvalitet, spårbarhet och personberoende blir den istället en möjliggörare för en enklare och mer tillförlitlig produktion."
  },
  {
    "objectID": "posts/Metadata/index.html",
    "href": "posts/Metadata/index.html",
    "title": "Metadata: före eller efter?",
    "section": "",
    "text": "Om du tänker att metadata är tidskrävande dokumentation eller ett grått tekniskt ämne så vill jag bara säga: det kan vara så – men det behöver inte vara så. Om man lägger upp arbetet rätt kan metadata till och med minska den totala dokumentationen, genom att reducera dubbelarbete och i slutändan också göra att produktionen följer dokumentationen snarare än tvärtom.\nTidigare tänkte jag inte särskilt mycket på metadata, utan var mer intresserad av analysarbete och de statistiska frågorna. Metadata kändes mest som en teknisk arkitekturfråga där värdet inte var helt uppenbart utöver att det hör till professionen.\nEfter att ha arbetat med statistikproduktion har jag insett att metadata har ett stort värde – och hur centralt arbetssättet kring metadata faktiskt är. Det stora värdet ligger i att metadata beskriver vad variabler egentligen betyder, exempelvis:\n\nvar de kommer ifrån\nvilken värdemängd de har\ni vilken skala de är mätta\nom de är härledda, och i så fall hur och från vilka variabler\nvilka variabler som är beroende av den specifika variabeln\n\nDet här är viktigt för användare av data, men också för produktionen i stort. Väl dokumenterad metadata minskar personberoende, gör förändringsarbete betydligt enklare och hör till professionell datahantering.\nSlutformatet kan till exempel vara en metadatakatalog i något system för enklare samkörning och säker lagring. Det betyder dock fortfarande att man antingen matar in saker manuellt eller gör någon form av mer eller mindre smart inläsning.\nHär tänkte jag resonera kring hur man kan arbeta med metadata: efter produktion eller före produktion."
  },
  {
    "objectID": "posts/Metadata/index.html#inledning-varför-metadata-ofta-blir-något-vid-sidan-av",
    "href": "posts/Metadata/index.html#inledning-varför-metadata-ofta-blir-något-vid-sidan-av",
    "title": "Metadata: före eller efter?",
    "section": "",
    "text": "Om du tänker att metadata är tidskrävande dokumentation eller ett grått tekniskt ämne så vill jag bara säga: det kan vara så – men det behöver inte vara så. Om man lägger upp arbetet rätt kan metadata till och med minska den totala dokumentationen, genom att reducera dubbelarbete och i slutändan också göra att produktionen följer dokumentationen snarare än tvärtom.\nTidigare tänkte jag inte särskilt mycket på metadata, utan var mer intresserad av analysarbete och de statistiska frågorna. Metadata kändes mest som en teknisk arkitekturfråga där värdet inte var helt uppenbart utöver att det hör till professionen.\nEfter att ha arbetat med statistikproduktion har jag insett att metadata har ett stort värde – och hur centralt arbetssättet kring metadata faktiskt är. Det stora värdet ligger i att metadata beskriver vad variabler egentligen betyder, exempelvis:\n\nvar de kommer ifrån\nvilken värdemängd de har\ni vilken skala de är mätta\nom de är härledda, och i så fall hur och från vilka variabler\nvilka variabler som är beroende av den specifika variabeln\n\nDet här är viktigt för användare av data, men också för produktionen i stort. Väl dokumenterad metadata minskar personberoende, gör förändringsarbete betydligt enklare och hör till professionell datahantering.\nSlutformatet kan till exempel vara en metadatakatalog i något system för enklare samkörning och säker lagring. Det betyder dock fortfarande att man antingen matar in saker manuellt eller gör någon form av mer eller mindre smart inläsning.\nHär tänkte jag resonera kring hur man kan arbeta med metadata: efter produktion eller före produktion."
  },
  {
    "objectID": "posts/Metadata/index.html#metadatadokumentation-i-efterhand",
    "href": "posts/Metadata/index.html#metadatadokumentation-i-efterhand",
    "title": "Metadata: före eller efter?",
    "section": "Metadatadokumentation i efterhand",
    "text": "Metadatadokumentation i efterhand\nKort sagt: Dokumentation i efterhand är enkelt att börja med – men riskerar att släpa efter över tid och kräver dubbelarbete.\nAtt dokumentera metadata i efterhand kräver minst planering: vi har data som input, datan bearbetas, outputen sparas – och sist dokumenterar vi den sparade outputdatan. Detta kan illustreras enligt flödesschemat nedan.\n\n\n\n\n\nflowchart LR\n  A[Input] --&gt; C[Databearbetning] --&gt;\n  id1[(Output)] -.-&gt; B[Metadatadokumnetation]\n\n\n\n\n\n\nI den enklaste formen skulle all metadata skrivas efteråt för alla variabler, kanske i stil med något liknande nedan:\n\n\n\n\n\nflowchart LR\nA[\n**Variabel 1**\n&Tab;Beskrivning: Utbildningsnivå\n&Tab;Typ: chr\n&Tab;Värdemängd:\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Låg\n&Tab;&Tab;&Tab;2: Medel\n&Tab;&Tab;&Tab;3: Hög\n&Tab;&Tab;&Tab;99: Okänt\n&Tab;Ursprung:\n&Tab;&Tab;Källa: Register\n&Tab;&Tab;&Tab;Register: Register XYZ\n&Tab;&Tab;&Tab;Registerversion: ÅÅÅ-MM-DD\n&Tab;#91;...#93;\n\n**Variabel 56**\n&Tab;Beskrivning: Mått på ...\n&Tab;Typ: int\n&Tab;Värdemängd:\n&Tab;&Tab;Min:\n&Tab;&Tab;&Tab;Minvärde: 0\n&Tab;&Tab;&Tab;Inklusivt: TRUE\n&Tab;&Tab;Max:\n&Tab;&Tab;&Tab;Maxvärde: FALSE\n&Tab;&Tab;&Tab;Inklusivt: NA\n&Tab;&Tab;Specialvärden:\n&Tab;&Tab;&Tab;-999: Partiellt bortfall\n&Tab;Ursprung:\n&Tab;&Tab;Källa: Härledd\n&Tab;&Tab;&Tab;Beroenda av: #91;Variabel 13, Variabel 16, ...#93; \n&Tab;&Tab;&Tab;Hur: func#40; #91;Variabel 13, Variabel 16, ...#93; #41;\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \n\n\n\n\n\n\nI vissa fall kan detta fungera helt okej – om produktion är av engångs- eller ad hoc-karaktär eller om saker inte förändras så mycket över tid. I praktiken brukar dock definitioner, kodning och härledningar förändras. Då kan man försöka dokumentera parallellt med omkodning i databearbetningen vilket är definitionen av dubbelarbete.\nDet är något effektivare, men innebär fortfarande ett extra lager av struktur och disciplin – och i praktiken gör man ofta samma sak två gånger. När detta arbetssätt inte upprätthålls finns risken att dokumentationen gradvis glider isär från den faktiska produktionen, vilket leder till lägre tillit till metadatat.\n\n\nEffektivisering med nycklar\nOavsett om man dokumenterar metadata före eller efter produktion är ett effektivt arbetssätt att använda nycklar.\nMed nycklar menar jag standardiserade koder som – när man kopplar ihop dokument – expanderar till mer detaljerade beskrivningar. I surveystatistik har man ofta samma svarsalternativ för många frågor. Istället för att upprepa samma värdemängd om och om igen kan man använda en gemensam kod som pekar på en central definition. Det minskar duplicering och gör ändringar i värdemängder potentiellt mer spårbara och kontrollerbara.\n\n\n\n\n\nflowchart LR\nA[\n*Variabel 1*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;#91;...#93;\n*Variabel 43*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \nB[\n*Värdemängdstabell*\n**Kod: JaNej**\n&Tab;Typ: chr\n&Tab;Värdemängd:\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Ja\n&Tab;&Tab;&Tab;2: Nej\n&Tab;&Tab;&Tab;3: Vet Ej\n&Tab;&Tab;&Tab;4: Vill inte svara\n&Tab;&Tab;&Tab;99: Partiellt bortfall\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \nC[\n*Variabel 1*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;&Tab;Typ: chr\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Ja\n&Tab;&Tab;&Tab;2: Nej\n&Tab;&Tab;&Tab;3: Vet Ej\n&Tab;&Tab;&Tab;4: Vill inte svara\n&Tab;&Tab;&Tab;99: Partiellt bortfall\n&Tab;#91;...#93;\n*Variabel 43*\n&Tab;Beskrivning: Fråga 13\n&Tab;Värdemängd:\n&Tab;&Tab;**Kod: JaNej**\n&Tab;&Tab;Typ: chr\n&Tab;&Tab;Värden:\n&Tab;&Tab;&Tab;1: Ja\n&Tab;&Tab;&Tab;2: Nej\n&Tab;&Tab;&Tab;3: Vet Ej\n&Tab;&Tab;&Tab;4: Vill inte svara\n&Tab;&Tab;&Tab;99: Partiellt bortfall\n&Tab;#91;...#93;\n]\nstyle A text-align:left\nstyle A white-space: pre; \nA --&gt; C\nstyle B text-align:left\nstyle B white-space: pre; \nB --&gt; C\nstyle C text-align:left\nstyle C white-space: pre; \n\n\n\n\n\n\nHar man mycket återkommande struktur lönar det sig nästan alltid att normalisera metadatat i separata tabeller. Har man metadata i en nestlad struktur kan man också enkelt plocka ut delmängder vid behov – till exempel för att generera en kodbok där variabelnamn, beskrivning och värdemängd ofta räcker långt."
  },
  {
    "objectID": "posts/Metadata/index.html#metadatadokumentation-innan-data",
    "href": "posts/Metadata/index.html#metadatadokumentation-innan-data",
    "title": "Metadata: före eller efter?",
    "section": "Metadatadokumentation innan data",
    "text": "Metadatadokumentation innan data\nKort sagt: Metadatastyrd produktion kräver mer initialt arbete – men betalar tillbaka sig i förvaltning och kvalitet.\nAtt dokumentera metadata innan data kan låta märkligt, men vid återkommande produktion är det ofta både smart, effektivt, förvaltningsbart och kvalitetssäkert: om man planerar för det!\nTanken är att göra produktionen metadatastyrd: koden byggs upp så att den läser in både data och metadata och bearbetar datan utifrån metadata.\nSchematiskt:\n\n\n\n\n\nflowchart LR\n  A[Input] --&gt; C[Databearbetning] --&gt;\n  id1[(Output)]\n  B[Metadatadokumnetation] --&gt; C\n\n\n\n\n\n\nFördelarna är tydliga:\n\nDokumentationen blir primär, inte sekundär.\nMetadatat kan inte släpa efter – produktionen kräver korrekt metadata.\nDubbelarbete försvinner.\nKoden blir generisk: istället för speciallogik per variabel räcker en funktion per moment.\nFörvaltningen blir enklare.\n\nMed ett sådant upplägg kan man till exempel:\n\nKontrollera indata genom villkor i metadatat.\nHärleda nya variabler genom att specificera beroenden och transformationsregler i metadata.\nStyra vilka variabler som ska användas för redovisning och skattning baserat på metadata.\nGöra systematiska slutkontroller av färdig data.\nDetta är bara några exempel – det finns mycket mer man kan göra med ett metadatadrivet angreppssätt.\n\nNackdelen är primärt att det kräver planering men också att funktioner som är generella tenderar att bli mindre läsvänliga och kan bli komplexa när bearbetning har beroende i många steg."
  },
  {
    "objectID": "posts/Metadata/index.html#avslutning-metadata-som-arbetssätt-inte-bihang",
    "href": "posts/Metadata/index.html#avslutning-metadata-som-arbetssätt-inte-bihang",
    "title": "Metadata: före eller efter?",
    "section": "Avslutning: metadata som arbetssätt, inte bihang",
    "text": "Avslutning: metadata som arbetssätt, inte bihang\nJag tycker det är intressant att se att metadata kan vara en del av själva produktionslogiken. Att dynamiken i en produktion kan förändras beroende på metadatans roll: från ett dokumentationssteg till styrande och själva hjärtat i flödet.\nDet betyder inte att metadatastyrda arbetssätt alltid är rätt väg. I mindre, engångsbetonade projekt är tröskeln ofta för hög i relation till nyttan. Men i återkommande produktion, där förändringar sker över tid och där flera personer är inblandade kan det vara värt att åtminstonde fundera på metadatans roll.\nPoängen är inte att bygga det perfekta metadatasystemet, utan att vara medveten om vilken roll metadata har och kan ha. Om metadata ses som ett nödvändigt ont kommer den sannolikt alltid att ligga efter. Om den ses som ett verktyg för att hantera kvalitet, spårbarhet och personberoende blir den istället en möjliggörare för en enklare och mer tillförlitlig produktion."
  }
]